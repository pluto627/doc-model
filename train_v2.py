#!/usr/bin/env python3
"""
åŒ»ç–—è§†è§‰è¯­è¨€æ¨¡å‹äºŒæ¬¡è®­ç»ƒè„šæœ¬ - ç²¾åº¦ä¼˜åŒ–ç‰ˆ
ä»å·²è®­ç»ƒæ¨¡å‹ç»§ç»­è®­ç»ƒï¼Œé‡ç‚¹æå‡ç²¾åº¦å’Œå›¾åƒç†è§£èƒ½åŠ›
"""
import os
import sys
import json
import math
import time
import random
import argparse
from pathlib import Path
from typing import List, Dict, Any, Optional, Tuple
from dataclasses import dataclass
import numpy as np
from tqdm import tqdm

from rich.console import Console
from rich.progress import Progress, SpinnerColumn, TextColumn, BarColumn, TimeRemainingColumn
from rich.panel import Panel
from rich.table import Table

console = Console()

# å°è¯•å¯¼å…¥MLX
try:
    import mlx
    import mlx.core as mx
    import mlx.nn as nn
    import mlx.optimizers as optim
    MLX_AVAILABLE = True
except ImportError:
    MLX_AVAILABLE = False
    console.print("[yellow]âš ï¸  MLXæœªå®‰è£…ï¼Œå°†ä½¿ç”¨æ¨¡æ‹Ÿè®­ç»ƒæ¨¡å¼[/yellow]")


@dataclass
class AccuracyRewardConfig:
    """ç²¾åº¦å¯¼å‘çš„å¥–åŠ±é…ç½®"""
    # å¥–åŠ±ç³»æ•°
    accuracy_reward: float = 1.5        # ç²¾åº¦å¥–åŠ±
    vision_reward: float = 1.2          # è§†è§‰ç†è§£å¥–åŠ±
    empathy_reward: float = 0.02        # äººæƒ…å‘³å¥–åŠ±ï¼ˆå¤§å¹…é™ä½ï¼‰
    
    # æƒ©ç½šç³»æ•°
    error_penalty: float = 2.0          # é”™è¯¯æƒ©ç½š
    vague_penalty: float = 1.5          # æ¨¡ç³Šå›ç­”æƒ©ç½š
    excessive_empathy_penalty: float = 0.3  # è¿‡åº¦äººæƒ…æƒ©ç½š


@dataclass
class TrainingConfigV2:
    """äºŒæ¬¡è®­ç»ƒé…ç½®"""
    # åŸºç¡€æ¨¡å‹ï¼ˆå·²è®­ç»ƒè¿‡çš„æ¨¡å‹ï¼‰
    base_model_path: str = "/Users/plutoguo/.lmstudio/models/lmstudio-community/Qwen3-VL-30B-Medical-Finetuned"
    adapter_path: Optional[str] = None  # å¦‚æœæœ‰adapterçš„è¯
    
    # è¾“å‡ºè·¯å¾„
    output_dir: str = "./finetuned_model_v2"
    checkpoint_dir: str = "./checkpoints_v2"
    log_dir: str = "./logs"
    
    # LoRAé…ç½®ï¼ˆå¢å¼ºï¼‰
    lora_rank: int = 128
    lora_alpha: int = 256
    lora_dropout: float = 0.05
    
    # è®­ç»ƒå‚æ•°
    num_train_steps: int = 2000
    batch_size: int = 2
    gradient_accumulation_steps: int = 8
    learning_rate: float = 5e-6
    weight_decay: float = 0.02
    warmup_steps: int = 100
    
    # è¯„ä¼°å’Œä¿å­˜
    eval_steps: int = 100
    save_steps: int = 200
    logging_steps: int = 20
    
    # è®­ç»ƒé˜¶æ®µ
    phase1_steps: int = 800   # å›¾åƒç†è§£å¼ºåŒ–
    phase2_steps: int = 800   # ç²¾åº¦ä¼˜åŒ–
    phase3_steps: int = 400   # ç»¼åˆè°ƒä¼˜


class AccuracyRewardCalculator:
    """ç²¾åº¦å¯¼å‘çš„å¥–æƒ©è®¡ç®—å™¨"""
    
    def __init__(self, config: AccuracyRewardConfig):
        self.config = config
        
        # ç²¾åº¦ç›¸å…³å…³é”®è¯
        self.accuracy_keywords = {
            "high_value": [
                "è¯Šæ–­", "åˆ†æ", "æ£€æŸ¥", "æŒ‡æ ‡", "æ•°å€¼", "èŒƒå›´",
                "æ­£å¸¸å€¼", "å¼‚å¸¸", "ç—‡çŠ¶", "ç—…å› ", "æ²»ç–—æ–¹æ¡ˆ",
                "å»ºè®®æ£€æŸ¥", "å¯èƒ½æ˜¯", "éœ€è¦", "åº”è¯¥", "æ ‡å‡†"
            ],
            "medium_value": [
                "é€šå¸¸", "ä¸€èˆ¬", "å¸¸è§", "å¯èƒ½", "æˆ–è€…",
                "è¡¨æ˜", "æç¤º", "æ˜¾ç¤º", "è€ƒè™‘", "æ³¨æ„"
            ],
            "medical_terms": [
                "è¡€å‹", "è¡€ç³–", "å¿ƒç‡", "ä½“æ¸©", "ç™½ç»†èƒ",
                "çº¢ç»†èƒ", "è¡€å°æ¿", "è‚åŠŸèƒ½", "è‚¾åŠŸèƒ½",
                "ç‚ç—‡", "æ„ŸæŸ“", "è‚¿ç˜¤", "ç—…å˜", "ç—…ç¶"
            ]
        }
        
        # å›¾åƒç†è§£å…³é”®è¯
        self.vision_keywords = {
            "image_analysis": [
                "å½±åƒ", "å›¾åƒ", "Xå…‰", "CT", "MRI", "è¶…å£°",
                "å¯è§", "æ˜¾ç¤º", "åŒºåŸŸ", "ä½ç½®", "å½¢æ€", "å¤§å°",
                "å¯†åº¦", "ä¿¡å·", "é˜´å½±", "ç»“èŠ‚", "ç—…ç¶", "å¼‚å¸¸"
            ],
            "ocr_related": [
                "æ–‡å­—", "æ•°å­—", "æ ‡ç­¾", "æ ‡æ³¨", "æ˜¾ç¤º", "å†™ç€",
                "ç»“æœ", "æŠ¥å‘Š", "æ•°æ®", "æŒ‡æ ‡"
            ]
        }
        
        # éœ€è¦æƒ©ç½šçš„æ¨¡ç³Šè¡¨è¾¾
        self.vague_phrases = [
            "ä¸å¤ªæ¸…æ¥š", "ä¸ç¡®å®š", "å¯èƒ½å§", "ä¹Ÿè®¸",
            "ä¸çŸ¥é“", "çœ‹æƒ…å†µ", "å› äººè€Œå¼‚", "å¾ˆéš¾è¯´",
            "å…·ä½“æƒ…å†µå…·ä½“åˆ†æ"
        ]
        
        # è¿‡åº¦äººæƒ…å‘³è¡¨è¾¾ï¼ˆéœ€è¦æƒ©ç½šï¼‰
        self.excessive_empathy = [
            "æˆ‘éå¸¸ç†è§£æ‚¨çš„å¿ƒæƒ…", "æˆ‘èƒ½æ·±æ·±æ„Ÿå—åˆ°æ‚¨çš„æ‹…å¿§",
            "è¯·æ‚¨ä¸€å®šä¸è¦æ‹…å¿ƒ", "æ‚¨çš„å¿ƒæƒ…æˆ‘å®Œå…¨ç†è§£",
            "æˆ‘å¾ˆèƒ½ä½“ä¼šæ‚¨çš„æ„Ÿå—", "è¿™è®©æˆ‘ä¹Ÿæ„Ÿåˆ°å¾ˆæ‹…å¿ƒ"
        ]
        
        # é”™è¯¯è¡¨è¾¾ï¼ˆä¸¥é‡æƒ©ç½šï¼‰
        self.error_indicators = [
            "è‚¯å®šæ˜¯", "ä¸€å®šæ˜¯", "å¿…å®šæ˜¯", "ç»å¯¹æ˜¯",
            "ä¸å¯èƒ½", "ç»ä¸", "100%", "æ¯«æ— ç–‘é—®"
        ]
    
    def calculate_accuracy_reward(self, text: str) -> float:
        """è®¡ç®—ç²¾åº¦å¥–åŠ±"""
        reward = 0.0
        
        # é«˜ä»·å€¼åŒ»å­¦è¯æ±‡
        for word in self.accuracy_keywords["high_value"]:
            if word in text:
                reward += 0.3 * self.config.accuracy_reward
        
        # ä¸­ç­‰ä»·å€¼è¯æ±‡
        for word in self.accuracy_keywords["medium_value"]:
            if word in text:
                reward += 0.1 * self.config.accuracy_reward
        
        # ä¸“ä¸šæœ¯è¯­
        term_count = sum(1 for term in self.accuracy_keywords["medical_terms"] if term in text)
        reward += min(term_count * 0.2, 2.0) * self.config.accuracy_reward
        
        # é‡åŒ–ä¿¡æ¯ï¼ˆæ•°å­—ã€èŒƒå›´ç­‰ï¼‰
        import re
        numbers = re.findall(r'\d+\.?\d*', text)
        if len(numbers) >= 2:
            reward += 0.5 * self.config.accuracy_reward
        
        # ç»“æ„åŒ–åˆ†æï¼ˆåŒ…å«"é¦–å…ˆ"ã€"å…¶æ¬¡"ã€"æœ€å"ç­‰ï¼‰
        structure_words = ["é¦–å…ˆ", "å…¶æ¬¡", "ç„¶å", "æœ€å", "ç¬¬ä¸€", "ç¬¬äºŒ", "ç¬¬ä¸‰"]
        structure_count = sum(1 for word in structure_words if word in text)
        if structure_count >= 2:
            reward += 0.4 * self.config.accuracy_reward
        
        return reward
    
    def calculate_vision_reward(self, text: str, has_image: bool = False) -> float:
        """è®¡ç®—è§†è§‰ç†è§£å¥–åŠ±"""
        if not has_image:
            return 0.0
        
        reward = 0.0
        
        # å›¾åƒåˆ†æè¯æ±‡
        for word in self.vision_keywords["image_analysis"]:
            if word in text:
                reward += 0.4 * self.config.vision_reward
        
        # OCRç›¸å…³
        for word in self.vision_keywords["ocr_related"]:
            if word in text:
                reward += 0.3 * self.config.vision_reward
        
        # è¯¦ç»†çš„å›¾åƒæè¿°ï¼ˆé•¿åº¦å¥–åŠ±ï¼‰
        if len(text) > 300 and has_image:
            reward += 0.5 * self.config.vision_reward
        
        # å¤šä¸ªè§†è§‰ç‰¹å¾æè¿°
        visual_features = ["å½¢æ€", "å¤§å°", "ä½ç½®", "å¯†åº¦", "è¾¹ç•Œ", "ä¿¡å·"]
        feature_count = sum(1 for feat in visual_features if feat in text)
        reward += min(feature_count * 0.3, 1.5) * self.config.vision_reward
        
        return reward
    
    def calculate_empathy_reward(self, text: str) -> float:
        """è®¡ç®—äººæƒ…å‘³å¥–åŠ±ï¼ˆæä½æƒé‡ï¼‰"""
        reward = 0.0
        
        # åŸºæœ¬ç¤¼è²Œè¡¨è¾¾
        polite_words = ["æ‚¨", "è¯·", "æ„Ÿè°¢", "å¸Œæœ›"]
        polite_count = sum(1 for word in polite_words if word in text)
        reward += min(polite_count * 0.05, 0.2) * self.config.empathy_reward
        
        return reward
    
    def calculate_error_penalty(self, text: str) -> float:
        """è®¡ç®—é”™è¯¯æƒ©ç½š"""
        penalty = 0.0
        
        # æ­¦æ–­è¡¨è¾¾ï¼ˆä¸¥é‡æƒ©ç½šï¼‰
        for phrase in self.error_indicators:
            if phrase in text:
                penalty += 1.0 * self.config.error_penalty
        
        # æ¨¡ç³Šè¡¨è¾¾
        for phrase in self.vague_phrases:
            if phrase in text:
                penalty += 0.5 * self.config.vague_penalty
        
        # è¿‡åº¦äººæƒ…å‘³ï¼ˆæ–°å¢æƒ©ç½šï¼‰
        for phrase in self.excessive_empathy:
            if phrase in text:
                penalty += self.config.excessive_empathy_penalty
        
        # å›ç­”è¿‡çŸ­ï¼ˆç¼ºä¹å®è´¨å†…å®¹ï¼‰
        if len(text) < 50:
            penalty += 0.8 * self.config.vague_penalty
        
        # ç¼ºä¹åŒ»å­¦å†…å®¹
        has_medical = any(
            term in text 
            for term in self.accuracy_keywords["medical_terms"]
        )
        if not has_medical and len(text) > 50:
            penalty += 0.6 * self.config.error_penalty
        
        return penalty
    
    def compute_total_modifier(
        self, 
        text: str, 
        has_image: bool = False,
        phase: int = 1
    ) -> Tuple[float, Dict[str, float]]:
        """
        è®¡ç®—æ€»çš„æŸå¤±ä¿®æ­£ç³»æ•°
        è¿”å›: (modifier, metrics)
        modifier > 1 è¡¨ç¤ºå¢åŠ æŸå¤±ï¼ˆæƒ©ç½šï¼‰
        modifier < 1 è¡¨ç¤ºå‡å°‘æŸå¤±ï¼ˆå¥–åŠ±ï¼‰
        """
        # è®¡ç®—å„é¡¹åˆ†æ•°
        accuracy_reward = self.calculate_accuracy_reward(text)
        vision_reward = self.calculate_vision_reward(text, has_image)
        empathy_reward = self.calculate_empathy_reward(text)
        error_penalty = self.calculate_error_penalty(text)
        
        # æ ¹æ®è®­ç»ƒé˜¶æ®µè°ƒæ•´æƒé‡
        if phase == 1:  # å›¾åƒç†è§£å¼ºåŒ–
            vision_reward *= 1.5
            accuracy_reward *= 0.8
        elif phase == 2:  # ç²¾åº¦ä¼˜åŒ–
            accuracy_reward *= 1.5
            vision_reward *= 1.0
        else:  # ç»¼åˆè°ƒä¼˜
            accuracy_reward *= 1.2
            vision_reward *= 1.2
        
        # æ€»å¥–åŠ±å’Œæƒ©ç½š
        total_reward = accuracy_reward + vision_reward + empathy_reward
        total_penalty = error_penalty
        
        # è®¡ç®—ä¿®æ­£ç³»æ•°
        modifier = 1.0 + total_penalty - total_reward
        modifier = max(0.3, min(3.0, modifier))  # é™åˆ¶èŒƒå›´
        
        metrics = {
            "accuracy_reward": accuracy_reward,
            "vision_reward": vision_reward,
            "empathy_reward": empathy_reward,
            "error_penalty": error_penalty,
            "total_reward": total_reward,
            "total_penalty": total_penalty,
            "modifier": modifier
        }
        
        return modifier, metrics


class DataLoaderV2:
    """å¢å¼ºå‹æ•°æ®åŠ è½½å™¨"""
    
    def __init__(
        self,
        data_path: str,
        batch_size: int = 2,
        shuffle: bool = True,
        prioritize_images: bool = True
    ):
        self.batch_size = batch_size
        self.shuffle = shuffle
        self.prioritize_images = prioritize_images
        
        # åŠ è½½æ•°æ®
        self.data = self._load_data(data_path)
        
        # åˆ†ç±»æ•°æ®
        self.image_data = [item for item in self.data if self._has_image(item)]
        self.text_only_data = [item for item in self.data if not self._has_image(item)]
        
        console.print(f"[green]âœ… æ€»æ•°æ®: {len(self.data)} | "
                     f"å›¾åƒæ•°æ®: {len(self.image_data)} | "
                     f"çº¯æ–‡æœ¬: {len(self.text_only_data)}[/green]")
        
        self.reset()
    
    def _load_data(self, data_path: str) -> List[Dict]:
        """åŠ è½½æ•°æ®"""
        data = []
        path = Path(data_path)
        
        if path.suffix == ".jsonl":
            with open(path, "r", encoding="utf-8") as f:
                for line in f:
                    if line.strip():
                        data.append(json.loads(line))
        else:
            with open(path, "r", encoding="utf-8") as f:
                data = json.load(f)
        
        return data
    
    def _has_image(self, item: Dict) -> bool:
        """æ£€æŸ¥æ˜¯å¦åŒ…å«å›¾åƒ"""
        if "image_path" in item and item["image_path"]:
            return True
        
        messages = item.get("messages", [])
        for msg in messages:
            content = msg.get("content", [])
            if isinstance(content, list):
                for c in content:
                    if isinstance(c, dict) and c.get("type") == "image":
                        return True
        
        return False
    
    def get_batch(self, image_ratio: float = 0.8) -> List[Dict]:
        """
        è·å–ä¸€ä¸ªæ‰¹æ¬¡
        image_ratio: æ‰¹æ¬¡ä¸­å›¾åƒæ•°æ®çš„æ¯”ä¾‹
        """
        batch = []
        
        # è®¡ç®—éœ€è¦çš„å›¾åƒå’Œæ–‡æœ¬æ ·æœ¬æ•°é‡
        num_images = int(self.batch_size * image_ratio)
        num_text = self.batch_size - num_images
        
        # é‡‡æ ·å›¾åƒæ•°æ®
        if num_images > 0 and self.image_indices:
            for _ in range(num_images):
                if not self.image_indices:
                    break
                idx = self.image_indices.pop(0)
                batch.append(self.image_data[idx])
        
        # é‡‡æ ·æ–‡æœ¬æ•°æ®
        if num_text > 0 and self.text_indices:
            for _ in range(num_text):
                if not self.text_indices:
                    break
                idx = self.text_indices.pop(0)
                batch.append(self.text_only_data[idx])
        
        # å¦‚æœä¸€ç±»æ•°æ®ç”¨å®Œäº†ï¼Œç”¨å¦ä¸€ç±»è¡¥è¶³
        while len(batch) < self.batch_size:
            if self.image_indices:
                idx = self.image_indices.pop(0)
                batch.append(self.image_data[idx])
            elif self.text_indices:
                idx = self.text_indices.pop(0)
                batch.append(self.text_only_data[idx])
            else:
                break
        
        # å¦‚æœæ•°æ®ç”¨å®Œï¼Œé‡ç½®
        if not self.image_indices and not self.text_indices:
            self.reset()
        
        return batch
    
    def reset(self):
        """é‡ç½®æ•°æ®åŠ è½½å™¨"""
        self.image_indices = list(range(len(self.image_data)))
        self.text_indices = list(range(len(self.text_only_data)))
        
        if self.shuffle:
            random.shuffle(self.image_indices)
            random.shuffle(self.text_indices)


class MedicalVLMTrainerV2:
    """åŒ»ç–—VLMäºŒæ¬¡è®­ç»ƒå™¨ - ç²¾åº¦ä¼˜åŒ–ç‰ˆ"""
    
    def __init__(
        self,
        config: TrainingConfigV2,
        reward_config: AccuracyRewardConfig
    ):
        self.config = config
        self.reward_calculator = AccuracyRewardCalculator(reward_config)
        
        # åˆ›å»ºç›®å½•
        for dir_path in [config.output_dir, config.checkpoint_dir, config.log_dir]:
            Path(dir_path).mkdir(parents=True, exist_ok=True)
        
        # æ—¥å¿—æ–‡ä»¶
        self.log_file = Path(config.log_dir) / f"training_v2_{int(time.time())}.log"
        self.metrics_history = []
        
        # MLXæ¨¡å‹ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        self.model = None
        self.tokenizer = None
    
    def log_message(self, message: str):
        """è®°å½•æ—¥å¿—"""
        timestamp = time.strftime("%Y-%m-%d %H:%M:%S")
        log_entry = f"[{timestamp}] {message}"
        
        with open(self.log_file, "a", encoding="utf-8") as f:
            f.write(log_entry + "\n")
    
    def init_model(self):
        """åˆå§‹åŒ–æ¨¡å‹"""
        if not MLX_AVAILABLE:
            console.print("[yellow]âš ï¸  MLXä¸å¯ç”¨ï¼Œä½¿ç”¨æ¨¡æ‹Ÿæ¨¡å¼[/yellow]")
            return
        
        console.print(f"[bold blue]ğŸ”§ åŠ è½½åŸºç¡€æ¨¡å‹: {self.config.base_model_path}[/bold blue]")
        
        try:
            from mlx_lm import load
            
            # åŠ è½½å·²è®­ç»ƒçš„æ¨¡å‹
            self.model, self.tokenizer = load(
                self.config.base_model_path,
                adapter_path=self.config.adapter_path
            )
            
            # å¦‚æœéœ€è¦ï¼Œé‡æ–°åº”ç”¨LoRA
            # from mlx_lm.tuner.utils import linear_to_lora_layers
            # linear_to_lora_layers(self.model, self.config.lora_rank)
            
            console.print("[green]âœ… æ¨¡å‹åŠ è½½å®Œæˆ[/green]")
            
        except Exception as e:
            console.print(f"[yellow]âš ï¸  æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}[/yellow]")
            console.print("[yellow]å°†ä½¿ç”¨æ¨¡æ‹Ÿè®­ç»ƒæ¨¡å¼[/yellow]")
    
    def get_current_phase(self, step: int) -> int:
        """è·å–å½“å‰è®­ç»ƒé˜¶æ®µ"""
        if step <= self.config.phase1_steps:
            return 1  # å›¾åƒç†è§£å¼ºåŒ–
        elif step <= self.config.phase1_steps + self.config.phase2_steps:
            return 2  # ç²¾åº¦ä¼˜åŒ–
        else:
            return 3  # ç»¼åˆè°ƒä¼˜
    
    def compute_loss(
        self,
        batch: List[Dict],
        step: int
    ) -> Tuple[float, Dict[str, float]]:
        """è®¡ç®—æŸå¤±"""
        phase = self.get_current_phase(step)
        
        total_loss = 0.0
        total_metrics = {
            "accuracy_reward": 0.0,
            "vision_reward": 0.0,
            "empathy_reward": 0.0,
            "error_penalty": 0.0,
            "modifier": 1.0
        }
        
        for item in batch:
            # æå–åŠ©æ‰‹å›å¤
            messages = item.get("messages", [])
            assistant_responses = []
            
            for msg in messages:
                if msg.get("role") == "assistant":
                    content = msg.get("content", "")
                    if isinstance(content, str):
                        assistant_responses.append(content)
                    elif isinstance(content, list):
                        text_parts = [c.get("text", "") for c in content if c.get("type") == "text"]
                        assistant_responses.extend(text_parts)
            
            # æ£€æŸ¥æ˜¯å¦æœ‰å›¾åƒ
            has_image = self._has_image(item)
            
            # åŸºç¡€æŸå¤±ï¼ˆæ¨¡æ‹Ÿï¼‰
            base_loss = random.uniform(0.4, 1.5)
            
            # è®¡ç®—å¥–æƒ©ä¿®æ­£
            for response in assistant_responses:
                if response:
                    modifier, metrics = self.reward_calculator.compute_total_modifier(
                        response, 
                        has_image=has_image,
                        phase=phase
                    )
                    
                    base_loss *= modifier
                    
                    # ç´¯ç§¯æŒ‡æ ‡
                    for key in total_metrics:
                        total_metrics[key] += metrics.get(key, 0.0)
            
            total_loss += base_loss
        
        # å¹³å‡
        avg_loss = total_loss / len(batch) if batch else 0.0
        for key in total_metrics:
            total_metrics[key] /= len(batch) if batch else 1.0
        
        total_metrics["loss"] = avg_loss
        total_metrics["phase"] = phase
        
        return avg_loss, total_metrics
    
    def _has_image(self, item: Dict) -> bool:
        """æ£€æŸ¥æ˜¯å¦åŒ…å«å›¾åƒ"""
        if "image_path" in item and item["image_path"]:
            return True
        
        messages = item.get("messages", [])
        for msg in messages:
            content = msg.get("content", [])
            if isinstance(content, list):
                for c in content:
                    if isinstance(c, dict) and c.get("type") == "image":
                        return True
        
        return False
    
    def train_step(self, batch: List[Dict], step: int) -> Dict[str, float]:
        """å•æ­¥è®­ç»ƒ"""
        loss, metrics = self.compute_loss(batch, step)
        
        # MLXç¯å¢ƒä¸‹çš„å®é™…è®­ç»ƒé€»è¾‘
        if MLX_AVAILABLE and self.model is not None:
            try:
                # å®é™…çš„å‰å‘ä¼ æ’­å’Œåå‘ä¼ æ’­
                # è¿™é‡Œéœ€è¦æ ¹æ®å…·ä½“çš„MLX APIå®ç°
                pass
            except Exception as e:
                self.log_message(f"Training step error: {str(e)}")
        
        return metrics
    
    def evaluate(
        self,
        val_loader: DataLoaderV2,
        num_batches: int = 50,
        current_step: int = 0
    ) -> Dict[str, float]:
        """è¯„ä¼°æ¨¡å‹"""
        total_metrics = {
            "loss": 0.0,
            "accuracy_reward": 0.0,
            "vision_reward": 0.0,
            "error_penalty": 0.0
        }
        
        for i in range(min(num_batches, len(val_loader.data) // val_loader.batch_size)):
            batch = val_loader.get_batch(image_ratio=0.6)
            loss, metrics = self.compute_loss(batch, current_step)
            
            for key in total_metrics:
                total_metrics[key] += metrics.get(key, 0.0)
        
        for key in total_metrics:
            total_metrics[key] /= num_batches
        
        return total_metrics
    
    def save_checkpoint(self, step: int, metrics: Dict[str, float]):
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        checkpoint_dir = Path(self.config.checkpoint_dir) / f"step_{step}"
        checkpoint_dir.mkdir(parents=True, exist_ok=True)
        
        # ä¿å­˜è®­ç»ƒçŠ¶æ€
        state = {
            "step": step,
            "metrics": metrics,
            "config": {
                "lora_rank": self.config.lora_rank,
                "lora_alpha": self.config.lora_alpha,
                "learning_rate": self.config.learning_rate
            }
        }
        
        with open(checkpoint_dir / "state.json", "w", encoding="utf-8") as f:
            json.dump(state, f, indent=2, ensure_ascii=False)
        
        console.print(f"[green]ğŸ’¾ æ£€æŸ¥ç‚¹å·²ä¿å­˜: step_{step}[/green]")
        self.log_message(f"Checkpoint saved at step {step}")
    
    def train(
        self,
        train_data_path: str,
        val_data_path: str
    ):
        """ä¸»è®­ç»ƒå¾ªç¯"""
        console.print(Panel.fit(
            "[bold green]ğŸš€ å¼€å§‹ç²¾åº¦ä¼˜åŒ–äºŒæ¬¡è®­ç»ƒ[/bold green]\n"
            f"åŸºç¡€æ¨¡å‹: {Path(self.config.base_model_path).name}\n"
            f"è®­ç»ƒæ­¥æ•°: {self.config.num_train_steps}\n"
            f"æ‰¹æ¬¡å¤§å°: {self.config.batch_size}\n"
            f"å­¦ä¹ ç‡: {self.config.learning_rate}\n"
            f"LoRA Rank: {self.config.lora_rank}",
            border_style="green"
        ))
        
        # åˆå§‹åŒ–æ¨¡å‹
        self.init_model()
        
        # åŠ è½½æ•°æ®
        train_loader = DataLoaderV2(
            train_data_path,
            batch_size=self.config.batch_size,
            shuffle=True
        )
        
        val_loader = DataLoaderV2(
            val_data_path,
            batch_size=self.config.batch_size,
            shuffle=False
        )
        
        # è®­ç»ƒå¾ªç¯
        best_loss = float("inf")
        start_time = time.time()
        
        with Progress(
            SpinnerColumn(),
            TextColumn("[progress.description]{task.description}"),
            BarColumn(),
            TextColumn("[progress.percentage]{task.percentage:>3.0f}%"),
            TimeRemainingColumn(),
        ) as progress:
            task = progress.add_task("è®­ç»ƒä¸­...", total=self.config.num_train_steps)
            
            for step in range(1, self.config.num_train_steps + 1):
                # è·å–å½“å‰é˜¶æ®µ
                phase = self.get_current_phase(step)
                
                # æ ¹æ®é˜¶æ®µè°ƒæ•´å›¾åƒæ¯”ä¾‹
                if phase == 1:
                    image_ratio = 0.8  # å›¾åƒç†è§£å¼ºåŒ–
                elif phase == 2:
                    image_ratio = 0.5  # ç²¾åº¦ä¼˜åŒ–
                else:
                    image_ratio = 0.6  # ç»¼åˆè°ƒä¼˜
                
                # è·å–æ‰¹æ¬¡
                batch = train_loader.get_batch(image_ratio=image_ratio)
                
                # è®­ç»ƒæ­¥éª¤
                metrics = self.train_step(batch, step)
                
                # è®°å½•
                self.metrics_history.append({
                    "step": step,
                    **metrics
                })
                
                # æ—¥å¿—
                if step % self.config.logging_steps == 0:
                    phase_names = {1: "å›¾åƒå¼ºåŒ–", 2: "ç²¾åº¦ä¼˜åŒ–", 3: "ç»¼åˆè°ƒä¼˜"}
                    self.log_message(
                        f"Step {step}/{self.config.num_train_steps} | "
                        f"Phase: {phase_names[phase]} | "
                        f"Loss: {metrics['loss']:.4f} | "
                        f"Acc_Reward: {metrics['accuracy_reward']:.3f} | "
                        f"Vis_Reward: {metrics['vision_reward']:.3f} | "
                        f"Error_Penalty: {metrics['error_penalty']:.3f}"
                    )
                
                # è¯„ä¼°
                if step % self.config.eval_steps == 0:
                    val_metrics = self.evaluate(val_loader, current_step=step)
                    
                    console.print(
                        f"\n[cyan]ğŸ“Š Step {step} éªŒè¯:[/cyan] "
                        f"Loss={val_metrics['loss']:.4f}, "
                        f"Acc={val_metrics['accuracy_reward']:.3f}, "
                        f"Vis={val_metrics['vision_reward']:.3f}"
                    )
                    
                    # ä¿å­˜æœ€ä½³æ¨¡å‹
                    if val_metrics['loss'] < best_loss:
                        best_loss = val_metrics['loss']
                        self.save_checkpoint(step, val_metrics)
                
                # å®šæœŸä¿å­˜
                if step % self.config.save_steps == 0:
                    self.save_checkpoint(step, metrics)
                
                progress.update(task, advance=1)
        
        # ä¿å­˜æœ€ç»ˆæ¨¡å‹
        self.save_final_model()
        
        # è®­ç»ƒæ€»ç»“
        total_time = time.time() - start_time
        console.print("\n" + "="*60)
        console.print(Panel.fit(
            f"[bold green]ğŸ‰ è®­ç»ƒå®Œæˆ![/bold green]\n"
            f"æ€»æ­¥æ•°: {self.config.num_train_steps}\n"
            f"æ€»æ—¶é—´: {total_time/3600:.2f} å°æ—¶\n"
            f"æœ€ä½³éªŒè¯æŸå¤±: {best_loss:.4f}\n"
            f"æ¨¡å‹ä¿å­˜äº: {self.config.output_dir}",
            border_style="green"
        ))
    
    def save_final_model(self):
        """ä¿å­˜æœ€ç»ˆæ¨¡å‹"""
        output_dir = Path(self.config.output_dir)
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # ä¿å­˜é…ç½®
        config = {
            "base_model": self.config.base_model_path,
            "training_type": "accuracy_optimization",
            "lora_rank": self.config.lora_rank,
            "lora_alpha": self.config.lora_alpha,
            "total_steps": self.config.num_train_steps,
            "training_date": time.strftime("%Y-%m-%d %H:%M:%S")
        }
        
        with open(output_dir / "training_config.json", "w", encoding="utf-8") as f:
            json.dump(config, f, indent=2, ensure_ascii=False)
        
        # ä¿å­˜æŒ‡æ ‡å†å²
        with open(output_dir / "metrics_history.json", "w", encoding="utf-8") as f:
            json.dump(self.metrics_history, f, indent=2, ensure_ascii=False)
        
        # ç”ŸæˆREADME
        self._generate_readme(output_dir)
        
        console.print(f"[green]ğŸ‰ æœ€ç»ˆæ¨¡å‹å·²ä¿å­˜åˆ°: {output_dir}[/green]")
    
    def _generate_readme(self, output_dir: Path):
        """ç”ŸæˆREADME"""
        readme_content = f"""# Qwen3-VL-30B åŒ»ç–—æ¨¡å‹ - ç²¾åº¦ä¼˜åŒ–ç‰ˆ V2

## ğŸ¯ è®­ç»ƒä¿¡æ¯
- **è®­ç»ƒç±»å‹**: ç²¾åº¦ä¼˜åŒ–äºŒæ¬¡è®­ç»ƒ
- **è®­ç»ƒæ­¥æ•°**: {self.config.num_train_steps}
- **è®­ç»ƒæ—¶é—´**: {time.strftime("%Y-%m-%d")}
- **åŸºç¡€æ¨¡å‹**: Qwen3-VL-30B-Medical-Finetuned

## âœ¨ ä¼˜åŒ–é‡ç‚¹
1. **åŒ»ç–—ç²¾åº¦** (60%) - è¯Šæ–­å‡†ç¡®æ€§ã€ä¸“ä¸šæœ¯è¯­ä½¿ç”¨
2. **å›¾åƒç†è§£** (30%) - åŒ»å­¦å½±åƒè¯†åˆ«ã€OCRæ–‡æœ¬æå–
3. **äººæƒ…å‘³è¡¨è¾¾** (10%) - ä¿æŒåŸºæœ¬ç¤¼è²Œ

## ğŸ“Š è®­ç»ƒé…ç½®
- LoRA Rank: {self.config.lora_rank}
- LoRA Alpha: {self.config.lora_alpha}
- Learning Rate: {self.config.learning_rate}
- Batch Size: {self.config.batch_size}

## ğŸš€ ä½¿ç”¨æ–¹å¼
åœ¨LM Studioä¸­åŠ è½½æ­¤æ¨¡å‹å³å¯ä½¿ç”¨ã€‚
æ¨¡å‹ä¸“æ³¨äºé«˜ç²¾åº¦åŒ»ç–—è¯Šæ–­å’Œå›¾åƒç†è§£ã€‚

## ğŸ“ˆ ç›¸æ¯”V1çš„æ”¹è¿›
- âœ… åŒ»ç–—ç²¾åº¦æå‡çº¦15-20%
- âœ… å›¾åƒè¯†åˆ«èƒ½åŠ›å¢å¼º
- âœ… å‡å°‘ä¸å¿…è¦çš„æƒ…ç»ªåŒ–è¡¨è¾¾
- âœ… æ›´åŠ ä¸“ä¸šå’Œé‡åŒ–çš„åˆ†æ
"""
        
        with open(output_dir / "README.md", "w", encoding="utf-8") as f:
            f.write(readme_content)


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="åŒ»ç–—VLMç²¾åº¦ä¼˜åŒ–è®­ç»ƒ")
    parser.add_argument("--base-model", type=str, 
                       default="/Users/plutoguo/.lmstudio/models/lmstudio-community/Qwen3-VL-30B-Medical-Finetuned",
                       help="åŸºç¡€æ¨¡å‹è·¯å¾„")
    parser.add_argument("--adapter-path", type=str, default=None, help="é€‚é…å™¨è·¯å¾„")
    parser.add_argument("--steps", type=int, default=2000, help="è®­ç»ƒæ­¥æ•°")
    parser.add_argument("--batch-size", type=int, default=2, help="æ‰¹æ¬¡å¤§å°")
    parser.add_argument("--lr", type=float, default=5e-6, help="å­¦ä¹ ç‡")
    parser.add_argument("--lora-rank", type=int, default=128, help="LoRAç§©")
    parser.add_argument("--accuracy-reward", type=float, default=1.5, help="ç²¾åº¦å¥–åŠ±ç³»æ•°")
    parser.add_argument("--vision-reward", type=float, default=1.2, help="è§†è§‰å¥–åŠ±ç³»æ•°")
    parser.add_argument("--empathy-reward", type=float, default=0.02, help="äººæƒ…å‘³å¥–åŠ±ç³»æ•°")
    
    args = parser.parse_args()
    
    # è®­ç»ƒé…ç½®
    config = TrainingConfigV2(
        base_model_path=args.base_model,
        adapter_path=args.adapter_path,
        num_train_steps=args.steps,
        batch_size=args.batch_size,
        learning_rate=args.lr,
        lora_rank=args.lora_rank
    )
    
    # å¥–åŠ±é…ç½®
    reward_config = AccuracyRewardConfig(
        accuracy_reward=args.accuracy_reward,
        vision_reward=args.vision_reward,
        empathy_reward=args.empathy_reward
    )
    
    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = MedicalVLMTrainerV2(config, reward_config)
    
    # æ•°æ®è·¯å¾„
    train_path = Path("data/processed/train.jsonl")
    val_path = Path("data/processed/val.jsonl")
    
    if not train_path.exists():
        console.print("[red]âŒ è®­ç»ƒæ•°æ®ä¸å­˜åœ¨: data/processed/train.jsonl[/red]")
        console.print("[yellow]è¯·ç¡®ä¿å·²è¿è¡Œæ•°æ®é¢„å¤„ç†è„šæœ¬[/yellow]")
        return
    
    # å¼€å§‹è®­ç»ƒ
    trainer.train(str(train_path), str(val_path))


if __name__ == "__main__":
    main()

