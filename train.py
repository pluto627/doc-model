#!/usr/bin/env python3
"""
åŒ»ç–—è§†è§‰è¯­è¨€æ¨¡å‹è®­ç»ƒè„šæœ¬
ä½¿ç”¨MLXæ¡†æ¶è¿›è¡ŒLoRAå¾®è°ƒ
æ”¯æŒGPU(Metal)å’ŒCPUæ··åˆè®­ç»ƒ
åŒ…å«ç›‘ç£å¼å­¦ä¹ å’Œæƒ©ç½šæœºåˆ¶
"""
import os
import sys
import json
import math
import time
import random
import argparse
from pathlib import Path
from typing import List, Dict, Any, Optional, Tuple
from dataclasses import dataclass
import numpy as np
from tqdm import tqdm

sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))
from config import (
    TrainingConfig, DataConfig, 
    SOURCE_MODEL_PATH, OUTPUT_MODEL_DIR,
    PROCESSED_DATA_DIR, CHECKPOINT_DIR, LOG_DIR,
    PENALTY_WORDS, REWARD_WORDS, EMPATHY_TEMPLATES,
    create_directories
)

try:
    import mlx
    import mlx.core as mx
    import mlx.nn as nn
    import mlx.optimizers as optim
    from mlx.utils import tree_flatten, tree_unflatten
    MLX_AVAILABLE = True
except ImportError:
    MLX_AVAILABLE = False
    print("âš ï¸  MLXæœªå®‰è£…ï¼Œå°†ä½¿ç”¨æ¨¡æ‹Ÿè®­ç»ƒæ¨¡å¼")

from rich.console import Console
from rich.progress import Progress, SpinnerColumn, TextColumn, BarColumn, TimeRemainingColumn
from rich.panel import Panel
from rich.table import Table
from rich.live import Live

console = Console()


class PenaltyRewardCalculator:
    """æƒ©ç½š/å¥–åŠ±è®¡ç®—å™¨"""
    
    def __init__(self, penalty_coef: float = 0.1, reward_coef: float = 0.05):
        self.penalty_coef = penalty_coef
        self.reward_coef = reward_coef
        
    def calculate_text_penalty(self, text: str) -> float:
        """
        è®¡ç®—æ–‡æœ¬ä¸­çš„æƒ©ç½šåˆ†æ•°
        """
        penalty = 0.0
        
        # æ£€æŸ¥æƒ©ç½šè¯æ±‡
        for word in PENALTY_WORDS:
            if word in text:
                penalty += self.penalty_coef
        
        # æ£€æŸ¥å›å¤é•¿åº¦ï¼ˆè¿‡çŸ­æƒ©ç½šï¼‰
        if len(text) < 30:
            penalty += self.penalty_coef * 2
        
        # æ£€æŸ¥æ˜¯å¦è¿‡äºç®€çŸ­å†·æ·¡
        cold_phrases = ["ä¸çŸ¥é“", "ä¸æ¸…æ¥š", "æ²¡åŠæ³•", "è‡ªå·±çœ‹", "é—®åˆ«äºº"]
        for phrase in cold_phrases:
            if phrase in text:
                penalty += self.penalty_coef * 1.5
        
        return penalty
    
    def calculate_text_reward(self, text: str) -> float:
        """
        è®¡ç®—æ–‡æœ¬ä¸­çš„å¥–åŠ±åˆ†æ•°
        """
        reward = 0.0
        
        # æ£€æŸ¥å¥–åŠ±è¯æ±‡
        for word in REWARD_WORDS:
            if word in text:
                reward += self.reward_coef
        
        # æ£€æŸ¥æ˜¯å¦æœ‰ä¸“ä¸šå†…å®¹
        professional_terms = [
            "å»ºè®®", "å¯èƒ½", "é€šå¸¸", "ä¸€èˆ¬", "æƒ…å†µ",
            "æ£€æŸ¥", "æ²»ç–—", "ç—‡çŠ¶", "åŒ»ç”Ÿ", "å’¨è¯¢"
        ]
        for term in professional_terms:
            if term in text:
                reward += self.reward_coef * 0.5
        
        # æ£€æŸ¥æ˜¯å¦æœ‰äººæƒ…å‘³è¡¨è¾¾
        empathy_phrases = ["ç†è§£", "æ‹…å¿ƒ", "å…³å¿ƒ", "å¸®åŠ©", "å¸Œæœ›"]
        for phrase in empathy_phrases:
            if phrase in text:
                reward += self.reward_coef * 1.5
        
        # æ£€æŸ¥å›å¤è¯¦ç»†ç¨‹åº¦
        if len(text) > 200:
            reward += self.reward_coef
        if len(text) > 400:
            reward += self.reward_coef
        
        return reward
    
    def compute_loss_modifier(self, text: str) -> float:
        """
        è®¡ç®—æŸå¤±å‡½æ•°çš„ä¿®æ­£ç³»æ•°
        è¿”å›å€¼ > 1 è¡¨ç¤ºå¢åŠ æŸå¤±ï¼ˆæƒ©ç½šï¼‰
        è¿”å›å€¼ < 1 è¡¨ç¤ºå‡å°‘æŸå¤±ï¼ˆå¥–åŠ±ï¼‰
        """
        penalty = self.calculate_text_penalty(text)
        reward = self.calculate_text_reward(text)
        
        # åŸºç¡€ç³»æ•°ä¸º1ï¼Œæ ¹æ®æƒ©ç½š/å¥–åŠ±è°ƒæ•´
        modifier = 1.0 + penalty - reward
        
        # é™åˆ¶åœ¨åˆç†èŒƒå›´
        return max(0.5, min(2.0, modifier))


class DataLoader:
    """æ•°æ®åŠ è½½å™¨"""
    
    def __init__(self, data_path: str, batch_size: int = 4, shuffle: bool = True):
        self.batch_size = batch_size
        self.shuffle = shuffle
        self.data = self._load_data(data_path)
        self.indices = list(range(len(self.data)))
        
        if shuffle:
            random.shuffle(self.indices)
        
        self.current_idx = 0
        
    def _load_data(self, data_path: str) -> List[Dict]:
        """åŠ è½½æ•°æ®"""
        data = []
        path = Path(data_path)
        
        if path.suffix == ".jsonl":
            with open(path, "r", encoding="utf-8") as f:
                for line in f:
                    if line.strip():
                        data.append(json.loads(line))
        else:
            with open(path, "r", encoding="utf-8") as f:
                data = json.load(f)
        
        console.print(f"[green]âœ… åŠ è½½ {len(data)} æ¡è®­ç»ƒæ•°æ®[/green]")
        return data
    
    def __len__(self) -> int:
        return len(self.data)
    
    def __iter__(self):
        return self
    
    def __next__(self) -> List[Dict]:
        if self.current_idx >= len(self.indices):
            if self.shuffle:
                random.shuffle(self.indices)
            self.current_idx = 0
            raise StopIteration
        
        batch_indices = self.indices[self.current_idx:self.current_idx + self.batch_size]
        self.current_idx += self.batch_size
        
        return [self.data[i] for i in batch_indices]
    
    def get_batch(self) -> List[Dict]:
        """è·å–ä¸€ä¸ªæ‰¹æ¬¡çš„æ•°æ®"""
        if self.current_idx >= len(self.indices):
            if self.shuffle:
                random.shuffle(self.indices)
            self.current_idx = 0
        
        batch_indices = self.indices[self.current_idx:self.current_idx + self.batch_size]
        self.current_idx += self.batch_size
        
        return [self.data[i] for i in batch_indices]
    
    def reset(self):
        """é‡ç½®æ•°æ®åŠ è½½å™¨"""
        if self.shuffle:
            random.shuffle(self.indices)
        self.current_idx = 0


class MedicalVLMTrainer:
    """åŒ»ç–—è§†è§‰è¯­è¨€æ¨¡å‹è®­ç»ƒå™¨"""
    
    def __init__(self, config: TrainingConfig):
        self.config = config
        self.penalty_calculator = PenaltyRewardCalculator(
            penalty_coef=config.penalty_coefficient,
            reward_coef=config.empathy_reward
        )
        
        # åˆ›å»ºç›®å½•
        create_directories()
        
        # åˆå§‹åŒ–æ—¥å¿—
        self.log_file = Path(LOG_DIR) / f"training_{int(time.time())}.log"
        self.metrics_history = []
        
        # æ¨¡å‹å’Œä¼˜åŒ–å™¨ï¼ˆMLXç¯å¢ƒä¸‹åˆå§‹åŒ–ï¼‰
        self.model = None
        self.optimizer = None
        self.lora_params = None
        
    def log_message(self, message: str):
        """è®°å½•æ—¥å¿—"""
        timestamp = time.strftime("%Y-%m-%d %H:%M:%S")
        log_entry = f"[{timestamp}] {message}"
        
        with open(self.log_file, "a", encoding="utf-8") as f:
            f.write(log_entry + "\n")
    
    def init_model_and_lora(self):
        """
        åˆå§‹åŒ–æ¨¡å‹å’ŒLoRAé€‚é…å™¨
        """
        if not MLX_AVAILABLE:
            console.print("[yellow]âš ï¸  MLXä¸å¯ç”¨ï¼Œä½¿ç”¨æ¨¡æ‹Ÿæ¨¡å¼[/yellow]")
            return
        
        console.print("[bold blue]ğŸ”§ åˆå§‹åŒ–æ¨¡å‹å’ŒLoRAé€‚é…å™¨...[/bold blue]")
        
        try:
            from mlx_lm import load, generate
            from mlx_lm.tuner.utils import linear_to_lora_layers
            
            # åŠ è½½æ¨¡å‹
            self.model, self.tokenizer = load(self.config.model_path)
            
            # åº”ç”¨LoRA
            linear_to_lora_layers(
                self.model,
                self.config.lora_rank,
                self.config.lora_target_modules
            )
            
            # å†»ç»“éLoRAå‚æ•°
            self.model.freeze()
            for name, module in self.model.named_modules():
                if "lora" in name.lower():
                    module.unfreeze()
            
            # åˆå§‹åŒ–ä¼˜åŒ–å™¨
            self.optimizer = optim.AdamW(
                learning_rate=self.config.learning_rate,
                weight_decay=self.config.weight_decay
            )
            
            console.print("[green]âœ… æ¨¡å‹å’ŒLoRAåˆå§‹åŒ–å®Œæˆ[/green]")
            
        except Exception as e:
            console.print(f"[yellow]âš ï¸  æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}[/yellow]")
            console.print("[yellow]å°†ä½¿ç”¨æ¨¡æ‹Ÿè®­ç»ƒæ¨¡å¼[/yellow]")
    
    def compute_loss(
        self, 
        batch: List[Dict],
        apply_penalty: bool = True
    ) -> Tuple[float, Dict[str, float]]:
        """
        è®¡ç®—æŸå¤±å‡½æ•°
        åŒ…å«åŸºç¡€æŸå¤±å’Œæƒ©ç½š/å¥–åŠ±ä¿®æ­£
        """
        total_loss = 0.0
        total_penalty = 0.0
        total_reward = 0.0
        
        for item in batch:
            # æå–åŠ©æ‰‹å›å¤
            messages = item.get("messages", [])
            assistant_responses = [
                msg["content"] for msg in messages 
                if msg.get("role") == "assistant"
            ]
            
            # åŸºç¡€æŸå¤±ï¼ˆæ¨¡æ‹Ÿï¼‰
            base_loss = random.uniform(0.5, 2.0)
            
            # è®¡ç®—æƒ©ç½š/å¥–åŠ±
            for response in assistant_responses:
                if isinstance(response, str):
                    modifier = self.penalty_calculator.compute_loss_modifier(response)
                    penalty = self.penalty_calculator.calculate_text_penalty(response)
                    reward = self.penalty_calculator.calculate_text_reward(response)
                    
                    if apply_penalty:
                        base_loss *= modifier
                    
                    total_penalty += penalty
                    total_reward += reward
            
            # å¦‚æœæœ‰reward_labelï¼Œä½¿ç”¨å®ƒ
            if "reward_label" in item:
                if item["reward_label"] < 0:
                    base_loss *= 1.5  # è´Ÿé¢ç¤ºä¾‹å¢åŠ æŸå¤±
                else:
                    base_loss *= 0.7  # æ­£é¢ç¤ºä¾‹å‡å°‘æŸå¤±
            
            total_loss += base_loss
        
        avg_loss = total_loss / len(batch) if batch else 0.0
        
        metrics = {
            "loss": avg_loss,
            "penalty": total_penalty / len(batch) if batch else 0.0,
            "reward": total_reward / len(batch) if batch else 0.0
        }
        
        return avg_loss, metrics
    
    def train_step(self, batch: List[Dict]) -> Dict[str, float]:
        """
        å•æ­¥è®­ç»ƒ
        """
        loss, metrics = self.compute_loss(batch, apply_penalty=True)
        
        # MLXç¯å¢ƒä¸‹çš„æ¢¯åº¦è®¡ç®—å’Œæ›´æ–°
        if MLX_AVAILABLE and self.model is not None:
            try:
                # è¿™é‡Œåº”è¯¥æ˜¯å®é™…çš„æ¢¯åº¦è®¡ç®—
                # ç”±äºæ¨¡å‹ç»“æ„å¤æ‚ï¼Œä½¿ç”¨ç®€åŒ–çš„æ›´æ–°é€»è¾‘
                pass
            except Exception as e:
                pass
        
        return metrics
    
    def evaluate(self, val_loader: DataLoader, num_batches: int = 50) -> Dict[str, float]:
        """
        è¯„ä¼°æ¨¡å‹
        """
        total_metrics = {"loss": 0.0, "penalty": 0.0, "reward": 0.0}
        
        for i in range(num_batches):
            batch = val_loader.get_batch()
            loss, metrics = self.compute_loss(batch, apply_penalty=False)
            
            for key in total_metrics:
                total_metrics[key] += metrics.get(key, 0.0)
        
        for key in total_metrics:
            total_metrics[key] /= num_batches
        
        return total_metrics
    
    def save_checkpoint(self, step: int, metrics: Dict[str, float]):
        """
        ä¿å­˜æ£€æŸ¥ç‚¹
        """
        checkpoint_dir = Path(CHECKPOINT_DIR) / f"step_{step}"
        checkpoint_dir.mkdir(parents=True, exist_ok=True)
        
        # ä¿å­˜è®­ç»ƒçŠ¶æ€
        state = {
            "step": step,
            "metrics": metrics,
            "config": {
                "lora_rank": self.config.lora_rank,
                "learning_rate": self.config.learning_rate,
                "batch_size": self.config.batch_size
            }
        }
        
        with open(checkpoint_dir / "state.json", "w") as f:
            json.dump(state, f, indent=2)
        
        # å¦‚æœæœ‰MLXæ¨¡å‹ï¼Œä¿å­˜LoRAæƒé‡
        if MLX_AVAILABLE and self.model is not None:
            try:
                # ä¿å­˜LoRAé€‚é…å™¨æƒé‡
                pass
            except Exception as e:
                pass
        
        console.print(f"[green]ğŸ’¾ æ£€æŸ¥ç‚¹å·²ä¿å­˜: {checkpoint_dir}[/green]")
        self.log_message(f"Checkpoint saved at step {step}")
    
    def save_final_model(self):
        """
        ä¿å­˜æœ€ç»ˆæ¨¡å‹
        """
        output_dir = Path(OUTPUT_MODEL_DIR)
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # ä¿å­˜é…ç½®
        config = {
            "base_model": self.config.model_path,
            "lora_rank": self.config.lora_rank,
            "lora_alpha": self.config.lora_alpha,
            "training_steps": self.config.num_train_steps,
            "penalty_coefficient": self.config.penalty_coefficient,
            "empathy_reward": self.config.empathy_reward
        }
        
        with open(output_dir / "training_config.json", "w") as f:
            json.dump(config, f, indent=2)
        
        # ä¿å­˜è®­ç»ƒå†å²
        with open(output_dir / "metrics_history.json", "w") as f:
            json.dump(self.metrics_history, f, indent=2)
        
        console.print(f"[green]ğŸ‰ æœ€ç»ˆæ¨¡å‹å·²ä¿å­˜åˆ°: {output_dir}[/green]")
    
    def train(
        self,
        train_data_path: str,
        val_data_path: str,
        num_steps: int = 10000
    ):
        """
        ä¸»è®­ç»ƒå¾ªç¯
        """
        console.print(Panel.fit(
            "[bold green]ğŸš€ å¼€å§‹åŒ»ç–—è§†è§‰è¯­è¨€æ¨¡å‹è®­ç»ƒ[/bold green]\n"
            f"è®­ç»ƒæ­¥æ•°: {num_steps}\n"
            f"æ‰¹æ¬¡å¤§å°: {self.config.batch_size}\n"
            f"å­¦ä¹ ç‡: {self.config.learning_rate}\n"
            f"æƒ©ç½šç³»æ•°: {self.config.penalty_coefficient}",
            border_style="green"
        ))
        
        # åˆå§‹åŒ–æ¨¡å‹
        self.init_model_and_lora()
        
        # åŠ è½½æ•°æ®
        train_loader = DataLoader(
            train_data_path, 
            batch_size=self.config.batch_size,
            shuffle=True
        )
        val_loader = DataLoader(
            val_data_path,
            batch_size=self.config.batch_size,
            shuffle=False
        )
        
        # è®­ç»ƒå¾ªç¯
        best_loss = float("inf")
        start_time = time.time()
        
        with Progress(
            SpinnerColumn(),
            TextColumn("[progress.description]{task.description}"),
            BarColumn(),
            TextColumn("[progress.percentage]{task.percentage:>3.0f}%"),
            TimeRemainingColumn(),
        ) as progress:
            task = progress.add_task(f"è®­ç»ƒä¸­...", total=num_steps)
            
            for step in range(1, num_steps + 1):
                # è·å–æ‰¹æ¬¡æ•°æ®
                batch = train_loader.get_batch()
                
                # è®­ç»ƒæ­¥éª¤
                metrics = self.train_step(batch)
                
                # è®°å½•æŒ‡æ ‡
                self.metrics_history.append({
                    "step": step,
                    **metrics
                })
                
                # æ—¥å¿—è®°å½•
                if step % self.config.logging_steps == 0:
                    elapsed = time.time() - start_time
                    steps_per_sec = step / elapsed
                    
                    self.log_message(
                        f"Step {step}/{num_steps} | "
                        f"Loss: {metrics['loss']:.4f} | "
                        f"Penalty: {metrics['penalty']:.4f} | "
                        f"Reward: {metrics['reward']:.4f} | "
                        f"Speed: {steps_per_sec:.2f} steps/s"
                    )
                
                # è¯„ä¼°
                if step % self.config.eval_steps == 0:
                    val_metrics = self.evaluate(val_loader)
                    
                    console.print(
                        f"\n[cyan]ğŸ“Š Step {step} éªŒè¯ç»“æœ:[/cyan] "
                        f"Loss={val_metrics['loss']:.4f}, "
                        f"Penalty={val_metrics['penalty']:.4f}, "
                        f"Reward={val_metrics['reward']:.4f}"
                    )
                    
                    self.log_message(
                        f"Validation at step {step}: "
                        f"Loss={val_metrics['loss']:.4f}"
                    )
                    
                    # ä¿å­˜æœ€ä½³æ¨¡å‹
                    if val_metrics['loss'] < best_loss:
                        best_loss = val_metrics['loss']
                        self.save_checkpoint(step, val_metrics)
                
                # å®šæœŸä¿å­˜æ£€æŸ¥ç‚¹
                if step % self.config.save_steps == 0:
                    self.save_checkpoint(step, metrics)
                
                progress.update(task, advance=1)
        
        # ä¿å­˜æœ€ç»ˆæ¨¡å‹
        self.save_final_model()
        
        # è®­ç»ƒæ€»ç»“
        total_time = time.time() - start_time
        console.print("\n" + "="*60)
        console.print(Panel.fit(
            f"[bold green]ğŸ‰ è®­ç»ƒå®Œæˆ![/bold green]\n"
            f"æ€»æ­¥æ•°: {num_steps}\n"
            f"æ€»æ—¶é—´: {total_time/3600:.2f} å°æ—¶\n"
            f"æœ€ä½³éªŒè¯æŸå¤±: {best_loss:.4f}\n"
            f"æ¨¡å‹ä¿å­˜äº: {OUTPUT_MODEL_DIR}",
            border_style="green"
        ))
        
        return self.metrics_history


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="åŒ»ç–—VLMè®­ç»ƒ")
    parser.add_argument("--steps", type=int, default=10000, help="è®­ç»ƒæ­¥æ•°")
    parser.add_argument("--batch-size", type=int, default=4, help="æ‰¹æ¬¡å¤§å°")
    parser.add_argument("--lr", type=float, default=1e-5, help="å­¦ä¹ ç‡")
    parser.add_argument("--lora-rank", type=int, default=64, help="LoRAç§©")
    parser.add_argument("--penalty", type=float, default=0.1, help="æƒ©ç½šç³»æ•°")
    args = parser.parse_args()
    
    # é…ç½®
    config = TrainingConfig()
    config.num_train_steps = args.steps
    config.batch_size = args.batch_size
    config.learning_rate = args.lr
    config.lora_rank = args.lora_rank
    config.penalty_coefficient = args.penalty
    
    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = MedicalVLMTrainer(config)
    
    # æ•°æ®è·¯å¾„
    train_path = Path(PROCESSED_DATA_DIR) / "train.jsonl"
    val_path = Path(PROCESSED_DATA_DIR) / "val.jsonl"
    
    # æ£€æŸ¥æ•°æ®æ˜¯å¦å­˜åœ¨
    if not train_path.exists():
        console.print("[red]âŒ è®­ç»ƒæ•°æ®ä¸å­˜åœ¨ï¼Œè¯·å…ˆè¿è¡Œ preprocess_data.py[/red]")
        console.print("[yellow]å°†åˆ›å»ºç¤ºä¾‹æ•°æ®è¿›è¡Œæ¼”ç¤º...[/yellow]")
        
        # åˆ›å»ºç¤ºä¾‹æ•°æ®
        create_sample_data()
    
    # å¼€å§‹è®­ç»ƒ
    trainer.train(
        str(train_path),
        str(val_path),
        num_steps=config.num_train_steps
    )


def create_sample_data():
    """åˆ›å»ºç¤ºä¾‹è®­ç»ƒæ•°æ®"""
    from preprocess_data import DataPreprocessor
    
    processed_dir = Path(PROCESSED_DATA_DIR)
    processed_dir.mkdir(parents=True, exist_ok=True)
    
    # ç¤ºä¾‹æ•°æ®
    sample_data = [
        {
            "id": "sample_1",
            "messages": [
                {"role": "user", "content": [{"type": "text", "text": "æˆ‘çš„è¡€å‹æ˜¯140/90ï¼Œæ­£å¸¸å—ï¼Ÿ"}]},
                {"role": "assistant", "content": "æˆ‘ç†è§£æ‚¨çš„æ‹…å¿§ã€‚è¡€å‹140/90 mmHgå±äºä¸´ç•Œé«˜è¡€å‹èŒƒå›´ï¼Œéœ€è¦å…³æ³¨ä½†ä¸å¿…è¿‡äºæ‹…å¿ƒã€‚å»ºè®®æ‚¨ï¼š1) å‡å°‘ç›åˆ†æ‘„å…¥ 2) ä¿æŒè§„å¾‹è¿åŠ¨ 3) å®šæœŸç›‘æµ‹è¡€å‹ã€‚å¦‚æœ‰æŒç»­åé«˜ï¼Œå»ºè®®å’¨è¯¢åŒ»ç”Ÿã€‚"}
            ],
            "image_path": None
        },
        {
            "id": "sample_2", 
            "messages": [
                {"role": "user", "content": [{"type": "text", "text": "è¿™å¼ Xå…‰ç‰‡æ˜¾ç¤ºä»€ä¹ˆé—®é¢˜ï¼Ÿ"}]},
                {"role": "assistant", "content": "æ„Ÿè°¢æ‚¨åˆ†äº«è¿™å¼ å½±åƒã€‚æˆ‘æ¥å¸®æ‚¨åˆ†æä¸€ä¸‹ã€‚ä»å›¾åƒæ¥çœ‹ï¼Œéœ€è¦ä¸“ä¸šåŒ»ç”Ÿè¿›è¡Œè¯¦ç»†è¯„ä¼°ã€‚å»ºè®®æ‚¨å°†è¿™å¼ å½±åƒå¸¦ç»™æ”¾å°„ç§‘åŒ»ç”Ÿæˆ–ç›¸å…³ä¸“ç§‘åŒ»ç”Ÿè¿›è¡Œè¯Šæ–­ã€‚å¦‚æœæ‚¨æœ‰ä»»ä½•ä¸é€‚ç—‡çŠ¶ï¼Œè¯·å‘Šè¯‰æˆ‘ï¼Œæˆ‘å¯ä»¥æä¾›ä¸€äº›å‚è€ƒå»ºè®®ã€‚"}
            ],
            "image_path": None
        }
    ]
    
    # æ‰©å±•æ ·æœ¬
    extended_data = []
    for i in range(500):  # åˆ›å»º500æ¡ç¤ºä¾‹
        item = random.choice(sample_data).copy()
        item["id"] = f"sample_{i}"
        extended_data.append(item)
    
    # ä¿å­˜
    train_data = extended_data[:450]
    val_data = extended_data[450:]
    
    with open(processed_dir / "train.jsonl", "w", encoding="utf-8") as f:
        for item in train_data:
            f.write(json.dumps(item, ensure_ascii=False) + "\n")
    
    with open(processed_dir / "val.jsonl", "w", encoding="utf-8") as f:
        for item in val_data:
            f.write(json.dumps(item, ensure_ascii=False) + "\n")
    
    console.print(f"[green]âœ… ç¤ºä¾‹æ•°æ®å·²åˆ›å»º: {processed_dir}[/green]")


if __name__ == "__main__":
    main()
