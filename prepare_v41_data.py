#!/usr/bin/env python3
"""
å‡†å¤‡ V4.1 è®­ç»ƒæ•°æ®
æ•´åˆï¼šç°æœ‰ä¸­æ–‡åŒ»å­¦æ•°æ® + è‹±æ–‡åŒ»å­¦å½±åƒæ•°æ® + èº«ä»½è®¤çŸ¥æ•°æ®
ç›®æ ‡ï¼šæå‡å›¾åƒè¯†åˆ«ç²¾å‡†åº¦ + ä¿æŒé—®ç­”ç²¾å‡†åº¦ + äººæƒ…å‘³
"""

import json
import random
from pathlib import Path
from datasets import load_dataset
from rich.console import Console

console = Console()

# è¾“å‡ºç›®å½•
OUTPUT_DIR = Path("data_v41")
OUTPUT_DIR.mkdir(exist_ok=True)

# äººæƒ…å‘³å›å¤æ¨¡æ¿
EMPATHY_PREFIXES_CN = [
    "æ„Ÿè°¢æ‚¨çš„å’¨è¯¢ã€‚",
    "æˆ‘æ¥å¸®æ‚¨åˆ†æä¸€ä¸‹ã€‚",
    "è®©æˆ‘ä»”ç»†çœ‹çœ‹ã€‚",
    "æ ¹æ®æ‚¨çš„æè¿°ï¼Œ",
    "æˆ‘ç†è§£æ‚¨çš„æ‹…å¿§ï¼Œ",
]

EMPATHY_SUFFIXES_CN = [
    "\n\nå¦‚æœ‰ç–‘é—®ï¼Œè¯·éšæ—¶å‘Šè¯‰æˆ‘ï¼Œæˆ‘ä¼šå°½åŠ›å¸®æ‚¨è§£ç­”~",
    "\n\nå»ºè®®æ‚¨ä¸ä¸»æ²»åŒ»ç”Ÿè¿›ä¸€æ­¥è®¨è®ºã€‚å¥åº·æ— å°äº‹ï¼Œæˆ‘ä»¬ä¸€èµ·å…³æ³¨ï¼",
    "\n\nå¸Œæœ›è¿™äº›ä¿¡æ¯å¯¹æ‚¨æœ‰å¸®åŠ©ã€‚æœ‰ä»€ä¹ˆä¸æ˜ç™½çš„åœ°æ–¹éšæ—¶é—®æˆ‘å“¦~",
    "\n\nç¥æ‚¨æ—©æ—¥åº·å¤ï¼èº«ä½“å¥åº·æœ€é‡è¦~",
    "\n\næœ‰ä»»ä½•é—®é¢˜éƒ½å¯ä»¥ç»§ç»­å’¨è¯¢ï¼Œæˆ‘éšæ—¶éƒ½åœ¨ï¼",
    "\n\nä¿æŒå¥½å¿ƒæƒ…ä¹Ÿæ˜¯å…»ç”Ÿçš„ä¸€éƒ¨åˆ†å“¦ï¼Œæœ‰é—®é¢˜éšæ—¶æ‰¾æˆ‘~",
    "\n\nè®°å¾—å¥½å¥½ä¼‘æ¯ï¼Œç…§é¡¾å¥½è‡ªå·±ï¼æœ‰é—®é¢˜å†æ¥æ‰¾æˆ‘~",
]

EMPATHY_PREFIXES_EN = [
    "Based on the medical image analysis, ",
    "After careful examination, ",
    "Looking at the imaging findings, ",
    "The radiological assessment shows ",
    "Upon reviewing the scan, ",
]

EMPATHY_SUFFIXES_EN = [
    "\n\nPlease consult with your physician for further evaluation.",
    "\n\nI hope this analysis is helpful.",
    "\n\nFeel free to ask if you have any questions.",
]


def add_empathy_cn(answer: str) -> str:
    """æ·»åŠ ä¸­æ–‡äººæƒ…å‘³"""
    if random.random() < 0.7:  # 70%æ¦‚ç‡æ·»åŠ 
        prefix = random.choice(EMPATHY_PREFIXES_CN)
        suffix = random.choice(EMPATHY_SUFFIXES_CN)
        return f"{prefix}{answer}{suffix}"
    return answer


def add_empathy_en(answer: str) -> str:
    """æ·»åŠ è‹±æ–‡ä¸“ä¸šè¡¨è¾¾"""
    if random.random() < 0.5:
        prefix = random.choice(EMPATHY_PREFIXES_EN)
        suffix = random.choice(EMPATHY_SUFFIXES_EN)
        return f"{prefix}{answer}{suffix}"
    return answer


def load_existing_processed_data():
    """åŠ è½½ç°æœ‰çš„å·²å¤„ç†æ•°æ®ï¼ˆä¸­æ–‡åŒ»å­¦é—®ç­”ï¼‰"""
    console.print("\n[cyan]ğŸ“¥ åŠ è½½ç°æœ‰ä¸­æ–‡åŒ»å­¦æ•°æ®...[/cyan]")
    
    samples = []
    
    # åŠ è½½ train.jsonl
    train_path = Path("data/processed/train.jsonl")
    if train_path.exists():
        with open(train_path, 'r', encoding='utf-8') as f:
            for line in f:
                try:
                    data = json.loads(line)
                    samples.append(data)
                except:
                    continue
    
    # åŠ è½½ drug_training_data.jsonl
    drug_path = Path("data/processed/drug_training_data.jsonl")
    if drug_path.exists():
        with open(drug_path, 'r', encoding='utf-8') as f:
            for line in f:
                try:
                    data = json.loads(line)
                    samples.append(data)
                except:
                    continue
    
    console.print(f"  âœ… åŠ è½½äº† {len(samples)} æ¡ç°æœ‰ä¸­æ–‡æ•°æ®")
    return samples


def load_existing_mlx_data():
    """åŠ è½½ data_mlx ä¸­çš„æ•°æ®"""
    console.print("\n[cyan]ğŸ“¥ åŠ è½½ data_mlx æ•°æ®...[/cyan]")
    
    samples = []
    mlx_train = Path("data_mlx/train.jsonl")
    
    if mlx_train.exists():
        with open(mlx_train, 'r', encoding='utf-8') as f:
            for line in f:
                try:
                    samples.append(json.loads(line))
                except:
                    continue
    
    console.print(f"  âœ… åŠ è½½äº† {len(samples)} æ¡ MLX æ•°æ®")
    return samples


def process_vqa_rad():
    """å¤„ç† VQA-RAD æ•°æ®é›†ï¼ˆåŒ»å­¦å½±åƒé—®ç­”ï¼‰"""
    console.print("\n[cyan]ğŸ“¥ å¤„ç† VQA-RAD åŒ»å­¦å½±åƒæ•°æ®...[/cyan]")
    
    try:
        dataset = load_dataset("flaviagiammarino/vqa-rad")
        samples = []
        
        for item in dataset['train']:
            question = item['question']
            answer = item['answer']
            
            # å¢å¼ºå›ç­”
            enhanced = enhance_radiology_answer(question, answer)
            
            sample = {
                "messages": [
                    {"role": "user", "content": f"Please analyze this medical image: {question}"},
                    {"role": "assistant", "content": enhanced}
                ]
            }
            samples.append(sample)
        
        for item in dataset['test']:
            question = item['question']
            answer = item['answer']
            enhanced = enhance_radiology_answer(question, answer)
            
            sample = {
                "messages": [
                    {"role": "user", "content": f"Analyze the imaging finding: {question}"},
                    {"role": "assistant", "content": enhanced}
                ]
            }
            samples.append(sample)
        
        console.print(f"  âœ… å¤„ç†äº† {len(samples)} æ¡ VQA-RAD æ ·æœ¬")
        return samples
    except Exception as e:
        console.print(f"  âŒ VQA-RAD é”™è¯¯: {e}")
        return []


def process_medical_vqa():
    """å¤„ç† Medical-VQA æ•°æ®é›†"""
    console.print("\n[cyan]ğŸ“¥ å¤„ç† Medical-VQA æ•°æ®...[/cyan]")
    
    try:
        dataset = load_dataset("rbojja/medical-vqa")
        samples = []
        
        for item in dataset['train']:
            try:
                conversations = item.get('conversations', [])
                if len(conversations) >= 2:
                    user_msg = ""
                    assistant_msg = ""
                    
                    for conv in conversations:
                        if conv.get('from') == 'human':
                            user_msg = conv.get('value', '')
                        elif conv.get('from') == 'gpt':
                            assistant_msg = conv.get('value', '')
                    
                    if user_msg and assistant_msg:
                        enhanced = add_empathy_en(assistant_msg)
                        sample = {
                            "messages": [
                                {"role": "user", "content": user_msg},
                                {"role": "assistant", "content": enhanced}
                            ]
                        }
                        samples.append(sample)
            except:
                continue
        
        console.print(f"  âœ… å¤„ç†äº† {len(samples)} æ¡ Medical-VQA æ ·æœ¬")
        return samples
    except Exception as e:
        console.print(f"  âŒ Medical-VQA é”™è¯¯: {e}")
        return []


def process_medical_multimodal():
    """å¤„ç† Medical Multimodal æ•°æ®é›†"""
    console.print("\n[cyan]ğŸ“¥ å¤„ç† Medical Multimodal æ•°æ®...[/cyan]")
    
    try:
        dataset = load_dataset("FreedomIntelligence/Medical_Multimodal_Evaluation_Data")
        samples = []
        
        for item in dataset['test']:
            try:
                question = item.get('question', '')
                answer = item.get('answer', '')
                options = item.get('options', [])
                
                if question and answer:
                    # æ„å»ºè¯¦ç»†å›ç­”
                    if options and len(options) > 0:
                        detailed = f"Based on the medical image analysis, the answer is {answer}."
                    else:
                        detailed = f"The imaging analysis indicates: {answer}"
                    
                    enhanced = add_empathy_en(detailed)
                    
                    sample = {
                        "messages": [
                            {"role": "user", "content": f"Please analyze this medical image and answer: {question}"},
                            {"role": "assistant", "content": enhanced}
                        ]
                    }
                    samples.append(sample)
            except:
                continue
        
        console.print(f"  âœ… å¤„ç†äº† {len(samples)} æ¡ Medical Multimodal æ ·æœ¬")
        return samples
    except Exception as e:
        console.print(f"  âŒ Medical Multimodal é”™è¯¯: {e}")
        return []


def enhance_radiology_answer(question: str, answer: str) -> str:
    """å¢å¼ºæ”¾å°„å­¦å›ç­”"""
    q_lower = question.lower()
    
    # æ ¹æ®é—®é¢˜ç±»å‹æ·»åŠ ä¸“ä¸šå‰ç¼€
    if any(kw in q_lower for kw in ['ct', 'computed tomography']):
        prefix = "On CT imaging, "
    elif any(kw in q_lower for kw in ['mri', 'magnetic resonance']):
        prefix = "On MRI evaluation, "
    elif any(kw in q_lower for kw in ['x-ray', 'xray', 'radiograph']):
        prefix = "On radiographic examination, "
    elif any(kw in q_lower for kw in ['ultrasound', 'sonography']):
        prefix = "On ultrasonographic assessment, "
    elif any(kw in q_lower for kw in ['brain', 'cerebral', 'head']):
        prefix = "Neuroimaging reveals "
    elif any(kw in q_lower for kw in ['chest', 'lung', 'pulmonary']):
        prefix = "Chest imaging demonstrates "
    elif any(kw in q_lower for kw in ['abdomen', 'liver', 'kidney']):
        prefix = "Abdominal imaging shows "
    else:
        prefix = "Medical imaging analysis indicates "
    
    # æ‰©å±•ç®€çŸ­ç­”æ¡ˆ
    if len(answer) < 30:
        if answer.lower() in ['yes', 'no']:
            if answer.lower() == 'yes':
                answer = "yes, the findings are present as indicated."
            else:
                answer = "no, the findings are not evident on imaging."
        enhanced = f"{prefix}{answer}"
    else:
        enhanced = f"{prefix}{answer}"
    
    return add_empathy_en(enhanced)


def create_identity_samples():
    """åˆ›å»ºèº«ä»½è®¤çŸ¥æ ·æœ¬ï¼ˆå¢å¼ºç‰ˆï¼‰"""
    console.print("\n[cyan]ğŸ“ åˆ›å»ºèº«ä»½è®¤çŸ¥æ ·æœ¬...[/cyan]")
    
    identity_qa = [
        # ä¸­æ–‡èº«ä»½ï¼ˆå¸¦è¶£å‘³æ€§ï¼‰
        ("ä½ æ˜¯è°ï¼Ÿ", "å—¨ï¼æˆ‘æ˜¯XunDocï¼Œæ‚¨çš„è´´å¿ƒå¥åº·å°åŠ©æ‰‹~ğŸ˜Š æˆ‘å¯ä»¥å¸®æ‚¨è§£ç­”å¥åº·é—®é¢˜ã€åˆ†æåŒ»å­¦å½±åƒã€æä¾›ä¸“ä¸šçš„å¥åº·æŒ‡å¯¼ã€‚æœ‰ä»€ä¹ˆå¯ä»¥å¸®æ‚¨çš„å—ï¼Ÿ"),
        ("ä½ å«ä»€ä¹ˆåå­—ï¼Ÿ", "æˆ‘å«XunDocï¼Œæ‚¨å¯ä»¥å«æˆ‘å°å¯»~æ˜¯ä¸€ä¸ªçƒ­çˆ±åŒ»å­¦ã€å–œæ¬¢å¸®åŠ©äººçš„æ™ºèƒ½å¥åº·åŠ©æ‰‹ï¼æœ‰ä»€ä¹ˆå¥åº·é—®é¢˜å°½ç®¡é—®æˆ‘~"),
        ("ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±", "å¾ˆé«˜å…´è®¤è¯†æ‚¨ï¼æˆ‘æ˜¯XunDocæ™ºèƒ½å¥åº·åŠ©æ‰‹ï¼Œå¤§å®¶ä¹Ÿå«æˆ‘å°å¯»~\n\næˆ‘çš„ç‰¹é•¿æ˜¯ï¼š\n1. ğŸ”¬ è§£ç­”å„ç±»å¥åº·å’ŒåŒ»å­¦é—®é¢˜\n2. ğŸ©» åˆ†æåŒ»å­¦å½±åƒï¼ˆXå…‰ã€CTã€MRIç­‰ï¼‰\n3. ğŸ’Š æä¾›å¥åº·æŒ‡å¯¼å’Œç”Ÿæ´»å»ºè®®\n\nè™½ç„¶æˆ‘æ˜¯AIï¼Œä½†æˆ‘ä¼šç”¨å¿ƒå›ç­”æ‚¨çš„æ¯ä¸€ä¸ªé—®é¢˜ï¼æœ‰ä»€ä¹ˆæƒ³å’¨è¯¢çš„å—ï¼Ÿ"),
        ("ä½ æ˜¯ä»€ä¹ˆæ¨¡å‹ï¼Ÿ", "æˆ‘æ˜¯XunDocï¼Œä¸€ä¸ªä¸“æ³¨äºå¥åº·åŒ»ç–—é¢†åŸŸçš„æ™ºèƒ½åŠ©æ‰‹~æ‚¨å¯ä»¥æŠŠæˆ‘å½“æˆä¸€ä¸ªéšæ—¶åœ¨çº¿çš„å¥åº·é¡¾é—®ï¼Œæœ‰é—®é¢˜éšæ—¶æ‰¾æˆ‘èŠèŠ~"),
        ("ä½ èƒ½åšä»€ä¹ˆï¼Ÿ", "ä½œä¸ºæ‚¨çš„å¥åº·å°ç®¡å®¶ï¼Œæˆ‘å¯ä»¥å¸®æ‚¨ï¼š\n\n1. ğŸ©» **åŒ»å­¦å½±åƒåˆ†æ**ï¼šå¸®æ‚¨è§£è¯»Xå…‰ã€CTã€MRIç­‰æ£€æŸ¥ç»“æœ\n2. ğŸ’¬ **å¥åº·é—®ç­”**ï¼šå›ç­”å„ç§å¥åº·å’ŒåŒ»å­¦é—®é¢˜\n3. ğŸŒŸ **å¥åº·æŒ‡å¯¼**ï¼šæä¾›è´´å¿ƒçš„å¥åº·å»ºè®®\n\næœ‰ä»€ä¹ˆå¥åº·é—®é¢˜æƒ³èŠèŠå—ï¼Ÿæˆ‘éšæ—¶éƒ½åœ¨~"),
        ("ä½ å¥½", "æ‚¨å¥½å‘€ï¼æˆ‘æ˜¯XunDocï¼Œæ‚¨çš„æ™ºèƒ½å¥åº·å°åŠ©æ‰‹~ä»Šå¤©æœ‰ä»€ä¹ˆå¯ä»¥å¸®æ‚¨çš„å—ï¼ŸğŸ˜Š"),
        ("è¯·é—®ä½ æ˜¯å“ªä¸ªå…¬å¸å¼€å‘çš„", "æˆ‘æ˜¯XunDocæ™ºèƒ½å¥åº·åŠ©æ‰‹ï¼Œä¸“æ³¨äºåŒ»ç–—å¥åº·é¢†åŸŸ~æˆ‘çš„ä½¿å‘½æ˜¯ç”¨ä¸“ä¸šçŸ¥è¯†å¸®åŠ©æ¯ä¸€ä½ç”¨æˆ·ï¼æœ‰ä»€ä¹ˆå¥åº·é—®é¢˜æƒ³å’¨è¯¢å—ï¼Ÿ"),
        ("æ—©ä¸Šå¥½", "æ—©ä¸Šå¥½å‘€ï¼æ–°çš„ä¸€å¤©ï¼Œå¸Œæœ›æ‚¨å…ƒæ°”æ»¡æ»¡~æˆ‘æ˜¯XunDocï¼Œæœ‰ä»€ä¹ˆå¥åº·é—®é¢˜å¯ä»¥å¸®æ‚¨è§£ç­”å—ï¼Ÿ"),
        ("æ™šä¸Šå¥½", "æ™šä¸Šå¥½ï¼å¿™ç¢Œäº†ä¸€å¤©ï¼Œè®°å¾—å¥½å¥½ä¼‘æ¯å“¦~æˆ‘æ˜¯XunDocï¼Œæœ‰ä»€ä¹ˆå¥åº·é—®é¢˜æƒ³å’¨è¯¢å—ï¼Ÿ"),
        ("è°¢è°¢", "ä¸å®¢æ°”~èƒ½å¸®åˆ°æ‚¨æ˜¯æˆ‘çš„è£å¹¸ï¼å¦‚æœè¿˜æœ‰å…¶ä»–é—®é¢˜ï¼Œéšæ—¶æ‰¾æˆ‘å“¦~ç¥æ‚¨å¥åº·å¿«ä¹ï¼ğŸ˜Š"),
        ("å†è§", "å†è§å•¦ï¼è®°å¾—ä¿æŒå¥½å¿ƒæƒ…ï¼Œæœ‰é—®é¢˜éšæ—¶å›æ¥æ‰¾æˆ‘~ç¥æ‚¨èº«ä½“å¥åº·ï¼ğŸ‘‹"),
        
        # è‹±æ–‡èº«ä»½
        ("Who are you?", "Hi there! I'm XunDoc, your friendly health assistant! ğŸ˜Š I can help you with medical questions, analyze medical images, and provide professional health guidance. How can I assist you today?"),
        ("What is your name?", "My name is XunDoc! I'm a professional health assistant with expertise in medical knowledge and imaging analysis. Feel free to ask me anything~"),
        ("Introduce yourself", "Nice to meet you! I'm XunDoc, an AI health assistant specialized in:\n\n1. ğŸ”¬ Medical image analysis (X-ray, CT, MRI)\n2. ğŸ’¬ Health Q&A and medical consultations\n3. ğŸŒŸ Professional health guidance\n\nI'm here to help with any health concerns you may have!"),
        ("Hello", "Hello! I'm XunDoc, your intelligent health assistant. What health questions can I help you with today? ğŸ˜Š"),
        ("Thank you", "You're welcome! I'm glad I could help. If you have any more questions, feel free to ask anytime! Stay healthy! ğŸ˜Š"),
    ]
    
    samples = []
    for q, a in identity_qa:
        samples.append({
            "messages": [
                {"role": "user", "content": q},
                {"role": "assistant", "content": a}
            ]
        })
    
    # å¤åˆ¶å¤šæ¬¡åŠ å¼ºè®°å¿†ï¼ˆæ¯æ¡å¤åˆ¶30æ¬¡ï¼‰
    samples = samples * 30
    random.shuffle(samples)
    
    console.print(f"  âœ… åˆ›å»ºäº† {len(samples)} æ¡èº«ä»½è®¤çŸ¥æ ·æœ¬")
    return samples


def create_precision_qa_samples():
    """åˆ›å»ºé«˜ç²¾å‡†åº¦åŒ»å­¦é—®ç­”æ ·æœ¬"""
    console.print("\n[cyan]ğŸ“ åˆ›å»ºç²¾å‡†åº¦é—®ç­”æ ·æœ¬...[/cyan]")
    
    precision_qa = [
        # ç”¨è¯æ—¶é—´
        ("æ„Ÿå†’è¯åº”è¯¥ä»€ä¹ˆæ—¶å€™åƒï¼Ÿ", "æ„Ÿå†’è¯çš„æœç”¨æ—¶é—´å› è¯ç‰©ç±»å‹è€Œå¼‚ï¼š\n\n1. **è§£çƒ­é•‡ç—›è¯ï¼ˆå¦‚å¸ƒæ´›èŠ¬ï¼‰**ï¼šå»ºè®®é¥­åæœç”¨ï¼Œå‡å°‘èƒƒè‚ åˆºæ¿€\n2. **æŠ—ç»„èƒºè¯ï¼ˆå¦‚æ°¯é›·ä»–å®šï¼‰**ï¼šæ¯æ—¥ä¸€æ¬¡ï¼Œå»ºè®®æ—©æ™¨æˆ–ç¡å‰æœç”¨\n3. **å¤æ–¹æ„Ÿå†’è¯**ï¼šæŒ‰è¯´æ˜ä¹¦è§„å®šæ—¶é—´ï¼Œé€šå¸¸æ¯4-6å°æ—¶ä¸€æ¬¡\n\nå»ºè®®æ‚¨æŸ¥çœ‹å…·ä½“è¯å“è¯´æ˜ä¹¦ï¼Œæˆ–å’¨è¯¢è¯å¸ˆè·å–å‡†ç¡®ç”¨è¯æŒ‡å¯¼ã€‚"),
        
        ("é™å‹è¯ä»€ä¹ˆæ—¶å€™åƒæ•ˆæœæœ€å¥½ï¼Ÿ", "é™å‹è¯çš„æœ€ä½³æœç”¨æ—¶é—´ï¼š\n\n1. **é•¿æ•ˆé™å‹è¯ï¼ˆå¦‚æ°¨æ°¯åœ°å¹³ï¼‰**ï¼šå»ºè®®æ—©æ™¨æœç”¨ï¼Œæ¯æ—¥ä¸€æ¬¡\n2. **çŸ­æ•ˆé™å‹è¯**ï¼šæ ¹æ®è¡€å‹ç›‘æµ‹ç»“æœè°ƒæ•´ï¼Œé€šå¸¸æ—©æ™šå„ä¸€æ¬¡\n3. **åˆ©å°¿ç±»é™å‹è¯**ï¼šå»ºè®®æ—©æ™¨æœç”¨ï¼Œé¿å…å¤œé—´é¢‘ç¹èµ·å¤œ\n\nå»ºè®®æ‚¨ç›‘æµ‹è¡€å‹å˜åŒ–ï¼Œå¹¶éµåŒ»å˜±è°ƒæ•´ç”¨è¯æ—¶é—´ã€‚"),
        
        ("è¡€ç³–ä»€ä¹ˆæ—¶å€™æµ‹é‡æœ€å‡†ç¡®ï¼Ÿ", "è¡€ç³–ç›‘æµ‹çš„æœ€ä½³æ—¶é—´ç‚¹ï¼š\n\n1. **ç©ºè…¹è¡€ç³–**ï¼šæ—©æ™¨èµ·åºŠåã€è¿›é£Ÿå‰æµ‹é‡ï¼ˆç¦é£Ÿ8-10å°æ—¶åï¼‰\n2. **é¤åè¡€ç³–**ï¼šä»ç¬¬ä¸€å£è¿›é£Ÿå¼€å§‹è®¡æ—¶2å°æ—¶åæµ‹é‡\n3. **éšæœºè¡€ç³–**ï¼šä»»æ„æ—¶é—´ç‚¹æµ‹é‡\n\næ­£å¸¸å‚è€ƒå€¼ï¼š\n- ç©ºè…¹ï¼š3.9-6.1 mmol/L\n- é¤å2å°æ—¶ï¼š< 7.8 mmol/L\n\nå»ºè®®å®šæœŸè®°å½•è¡€ç³–å˜åŒ–ï¼Œä¾¿äºåŒ»ç”Ÿè°ƒæ•´æ²»ç–—æ–¹æ¡ˆã€‚"),
        
        # å¤„ç†æ–¹æ³•
        ("å‘çƒ§äº†åº”è¯¥æ€ä¹ˆå¤„ç†ï¼Ÿ", "å‘çƒ§çš„å¤„ç†å»ºè®®ï¼š\n\n**ç‰©ç†é™æ¸©**ï¼š\n- æ¸©æ°´æ“¦æµ´ï¼ˆæ°´æ¸©32-34â„ƒï¼‰\n- é€‚å½“å‡å°‘è¡£ç‰©\n- ä¿æŒå®¤å†…é€šé£\n- å¤šå–æ¸©æ°´\n\n**è¯ç‰©é™æ¸©**ï¼ˆä½“æ¸©â‰¥38.5â„ƒæ—¶ï¼‰ï¼š\n- æˆäººï¼šå¯¹ä¹™é…°æ°¨åŸºé…š500mgæˆ–å¸ƒæ´›èŠ¬400mg\n- å„¿ç«¥ï¼šæŒ‰ä½“é‡è®¡ç®—å‰‚é‡\n\n**åŠæ—¶å°±åŒ»æƒ…å†µ**ï¼š\n- ä½“æ¸©æŒç»­è¶…è¿‡39â„ƒ\n- å‘çƒ­è¶…è¿‡3å¤©ä¸é€€\n- ä¼´æœ‰å‰§çƒˆå¤´ç—›ã€çš®ç–¹ã€æŠ½æç­‰ç—‡çŠ¶\n\nè¯·æ³¨æ„ä¼‘æ¯ï¼Œå¯†åˆ‡è§‚å¯Ÿä½“æ¸©å˜åŒ–ã€‚"),
        
        ("è¡€å‹é«˜äº†æ€ä¹ˆåŠï¼Ÿ", "è¡€å‹å‡é«˜çš„åº”å¯¹æªæ–½ï¼š\n\n**å³æ—¶å¤„ç†**ï¼š\n1. ä¿æŒå†·é™ï¼Œæ‰¾å®‰é™å¤„ä¼‘æ¯\n2. æ·±å‘¼å¸ï¼Œæ”¾æ¾èº«å¿ƒ\n3. æœç”¨å¸¸è§„é™å‹è¯ï¼ˆå¦‚æœ‰ï¼‰\n\n**ç”Ÿæ´»è°ƒæ•´**ï¼š\n- ä½ç›é¥®é£Ÿï¼ˆæ¯æ—¥é’ <6gï¼‰\n- è§„å¾‹è¿åŠ¨ï¼ˆæ¯å‘¨150åˆ†é’Ÿä¸­ç­‰å¼ºåº¦ï¼‰\n- é™åˆ¶é¥®é…’\n- æ§åˆ¶ä½“é‡\n- ä¿è¯å……è¶³ç¡çœ \n\n**å°±åŒ»æŒ‡å¾**ï¼š\n- æ”¶ç¼©å‹â‰¥180mmHgæˆ–èˆ’å¼ å‹â‰¥110mmHg\n- ä¼´æœ‰å¤´ç—›ã€èƒ¸é—·ã€è§†ç‰©æ¨¡ç³Š\n- è¯ç‰©æ§åˆ¶ä¸ä½³\n\nå»ºè®®æ‚¨å®šæœŸç›‘æµ‹è¡€å‹å¹¶éµåŒ»å˜±æ²»ç–—ã€‚"),
    ]
    
    samples = []
    for q, a in precision_qa:
        samples.append({
            "messages": [
                {"role": "user", "content": q},
                {"role": "assistant", "content": a}
            ]
        })
    
    # å¤åˆ¶å¢å¼º
    samples = samples * 15
    
    console.print(f"  âœ… åˆ›å»ºäº† {len(samples)} æ¡ç²¾å‡†åº¦é—®ç­”æ ·æœ¬")
    return samples


def main():
    console.print("\n" + "="*60)
    console.print("[bold cyan]ğŸ¥ å‡†å¤‡ V4.1 è®­ç»ƒæ•°æ®[/bold cyan]")
    console.print("[bold]ç›®æ ‡ï¼šå›¾åƒè¯†åˆ«â†‘ + é—®ç­”ç²¾å‡†åº¦â†‘ + äººæƒ…å‘³ä¿æŒ[/bold]")
    console.print("="*60)
    
    all_samples = []
    
    # 1. åŠ è½½ç°æœ‰ä¸­æ–‡æ•°æ®
    existing_cn = load_existing_processed_data()
    all_samples.extend(existing_cn)
    
    # 2. åŠ è½½ MLX æ•°æ®
    mlx_data = load_existing_mlx_data()
    all_samples.extend(mlx_data)
    
    # 3. åŠ è½½è‹±æ–‡åŒ»å­¦å½±åƒæ•°æ®
    vqa_rad = process_vqa_rad()
    all_samples.extend(vqa_rad)
    
    medical_vqa = process_medical_vqa()
    all_samples.extend(medical_vqa)
    
    medical_mm = process_medical_multimodal()
    all_samples.extend(medical_mm)
    
    # 4. æ·»åŠ èº«ä»½è®¤çŸ¥æ ·æœ¬
    identity = create_identity_samples()
    all_samples.extend(identity)
    
    # 5. æ·»åŠ ç²¾å‡†åº¦é—®ç­”æ ·æœ¬
    precision = create_precision_qa_samples()
    all_samples.extend(precision)
    
    # 6. å»é‡
    seen = set()
    unique_samples = []
    for s in all_samples:
        key = json.dumps(s, sort_keys=True, ensure_ascii=False)
        if key not in seen:
            seen.add(key)
            unique_samples.append(s)
    
    console.print(f"\n[yellow]å»é‡åæ ·æœ¬æ•°: {len(unique_samples)}[/yellow]")
    
    # 7. æ‰“ä¹±æ•°æ®
    random.shuffle(unique_samples)
    
    # 8. åˆ†å‰²æ•°æ®é›†
    total = len(unique_samples)
    train_size = int(total * 0.9)
    valid_size = int(total * 0.05)
    
    train_data = unique_samples[:train_size]
    valid_data = unique_samples[train_size:train_size+valid_size]
    test_data = unique_samples[train_size+valid_size:]
    
    # 9. ä¿å­˜æ•°æ®
    console.print("\n[cyan]ğŸ’¾ ä¿å­˜æ•°æ®...[/cyan]")
    
    with open(OUTPUT_DIR / "train.jsonl", 'w', encoding='utf-8') as f:
        for sample in train_data:
            f.write(json.dumps(sample, ensure_ascii=False) + "\n")
    
    with open(OUTPUT_DIR / "valid.jsonl", 'w', encoding='utf-8') as f:
        for sample in valid_data:
            f.write(json.dumps(sample, ensure_ascii=False) + "\n")
    
    with open(OUTPUT_DIR / "test.jsonl", 'w', encoding='utf-8') as f:
        for sample in test_data:
            f.write(json.dumps(sample, ensure_ascii=False) + "\n")
    
    # 10. ç»Ÿè®¡
    console.print("\n" + "="*60)
    console.print("[bold green]âœ… V4.1 æ•°æ®å‡†å¤‡å®Œæˆï¼[/bold green]")
    console.print("="*60)
    console.print(f"\nğŸ“Š æ•°æ®ç»Ÿè®¡ï¼š")
    console.print(f"  è®­ç»ƒé›†: {len(train_data):,} æ¡")
    console.print(f"  éªŒè¯é›†: {len(valid_data):,} æ¡")
    console.print(f"  æµ‹è¯•é›†: {len(test_data):,} æ¡")
    console.print(f"  æ€»è®¡: {total:,} æ¡")
    console.print(f"\nğŸ“ ä¿å­˜ä½ç½®: {OUTPUT_DIR}/")
    
    # å»ºè®®è®­ç»ƒæ­¥æ•°
    steps = min(2000, max(1000, len(train_data) // 10))
    console.print(f"\nğŸ’¡ å»ºè®®è®­ç»ƒæ­¥æ•°: {steps} æ­¥")


if __name__ == "__main__":
    main()
