#!/usr/bin/env python3
"""
æ•°æ®é¢„å¤„ç†è„šæœ¬
å¤„ç†åŒ»ç–—è§†è§‰è¯­è¨€æ•°æ®ï¼Œå‡†å¤‡è®­ç»ƒæ ¼å¼
"""
import os
import sys
import json
import random
from pathlib import Path
from typing import List, Dict, Any, Optional, Tuple
from PIL import Image
import numpy as np
from tqdm import tqdm
from concurrent.futures import ThreadPoolExecutor, as_completed

sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))
from config import (
    RAW_DATA_DIR, PROCESSED_DATA_DIR, 
    DataConfig, EMPATHY_TEMPLATES, 
    STYLE_GUIDELINES, PENALTY_WORDS, REWARD_WORDS,
    create_directories
)

from rich.console import Console
from rich.progress import Progress, SpinnerColumn, TextColumn, BarColumn
from rich.panel import Panel

console = Console()


class DataPreprocessor:
    """æ•°æ®é¢„å¤„ç†å™¨"""
    
    def __init__(self, config: DataConfig = None):
        self.config = config or DataConfig()
        self.raw_dir = Path(self.config.raw_data_dir)
        self.processed_dir = Path(self.config.processed_data_dir)
        self.processed_dir.mkdir(parents=True, exist_ok=True)
        
    def process_image(self, image_path: str, output_path: str = None) -> Optional[str]:
        """
        å¤„ç†å•å¼ å›¾åƒ
        - è°ƒæ•´å¤§å°
        - æ ‡å‡†åŒ–
        - ä¿å­˜ä¸ºç»Ÿä¸€æ ¼å¼
        """
        try:
            if not os.path.exists(image_path):
                return None
                
            img = Image.open(image_path)
            
            # è½¬æ¢ä¸ºRGB
            if img.mode != "RGB":
                img = img.convert("RGB")
            
            # è°ƒæ•´å¤§å°ï¼Œä¿æŒå®½é«˜æ¯”
            max_size = self.config.image_size
            ratio = min(max_size / img.width, max_size / img.height)
            
            if ratio < 1:
                new_size = (int(img.width * ratio), int(img.height * ratio))
                img = img.resize(new_size, Image.Resampling.LANCZOS)
            
            # ä¿å­˜
            if output_path:
                img.save(output_path, "JPEG", quality=95)
                return output_path
            
            return image_path
            
        except Exception as e:
            console.print(f"[yellow]âš ï¸ å›¾åƒå¤„ç†å¤±è´¥ {image_path}: {str(e)}[/yellow]")
            return None
    
    def enhance_response_with_empathy(self, response: str) -> str:
        """
        å¢å¼ºå›å¤çš„äººæƒ…å‘³
        """
        # æ£€æŸ¥æ˜¯å¦å·²ç»æœ‰äººæƒ…å‘³è¡¨è¾¾
        has_empathy = any(word in response for word in REWARD_WORDS[:3])
        
        if not has_empathy and len(response) > 50:
            # éšæœºæ·»åŠ äººæƒ…å‘³å¼€å¤´
            prefix = random.choice(EMPATHY_TEMPLATES)
            response = f"{prefix}\n\n{response}"
        
        # æ·»åŠ ç»“å°¾å…³æ€€
        if "?" not in response[-50:] and len(response) > 100:
            endings = [
                "\n\nå¦‚æœ‰å…¶ä»–ç–‘é—®ï¼Œè¯·éšæ—¶å‘Šè¯‰æˆ‘ã€‚",
                "\n\nå¸Œæœ›è¿™äº›ä¿¡æ¯å¯¹æ‚¨æœ‰å¸®åŠ©ã€‚",
                "\n\nå¦‚æœæ‚¨è¿˜æœ‰å…¶ä»–é—®é¢˜ï¼Œæˆ‘å¾ˆä¹æ„ä¸ºæ‚¨è§£ç­”ã€‚",
                "\n\nè¯·æ”¾å¿ƒï¼Œæœ‰ä»»ä½•é—®é¢˜éƒ½å¯ä»¥é—®æˆ‘ã€‚"
            ]
            response += random.choice(endings)
        
        return response
    
    def check_response_quality(self, response: str) -> Tuple[float, List[str]]:
        """
        æ£€æŸ¥å›å¤è´¨é‡ï¼Œè¿”å›æƒ©ç½šåˆ†æ•°å’Œé—®é¢˜åˆ—è¡¨
        """
        penalty_score = 0.0
        issues = []
        
        # æ£€æŸ¥æƒ©ç½šè¯æ±‡
        for word in PENALTY_WORDS:
            if word in response:
                penalty_score += 0.1
                issues.append(f"åŒ…å«è¿‡äºæ­¦æ–­çš„è¡¨è¾¾: '{word}'")
        
        # æ£€æŸ¥å›å¤é•¿åº¦
        if len(response) < 50:
            penalty_score += 0.2
            issues.append("å›å¤è¿‡çŸ­ï¼Œç¼ºä¹è¯¦ç»†è§£é‡Š")
        
        # æ£€æŸ¥æ˜¯å¦æœ‰ä¸“ä¸šå†…å®¹
        medical_keywords = ["å»ºè®®", "å¯èƒ½", "æƒ…å†µ", "æ£€æŸ¥", "æ²»ç–—", "ç—‡çŠ¶", "åŒ»ç”Ÿ"]
        has_medical_content = any(word in response for word in medical_keywords)
        if not has_medical_content:
            penalty_score += 0.1
            issues.append("ç¼ºä¹ä¸“ä¸šåŒ»ç–—å†…å®¹")
        
        # å¥–åŠ±äººæƒ…å‘³è¡¨è¾¾
        reward_count = sum(1 for word in REWARD_WORDS if word in response)
        penalty_score -= reward_count * 0.05
        
        return max(0, penalty_score), issues
    
    def format_conversation_for_training(
        self, 
        conversations: List[Dict], 
        image_path: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        å°†å¯¹è¯æ ¼å¼åŒ–ä¸ºè®­ç»ƒæ ¼å¼
        """
        # æ„å»ºQwen3-VLæ ¼å¼
        messages = []
        
        for conv in conversations:
            role = conv.get("role", "user")
            content = conv.get("content", "")
            
            if role == "user":
                msg_content = []
                
                # å¦‚æœæœ‰å›¾åƒï¼Œæ·»åŠ å›¾åƒæ ‡è®°
                if image_path and messages == []:  # åªåœ¨ç¬¬ä¸€ä¸ªç”¨æˆ·æ¶ˆæ¯æ·»åŠ å›¾åƒ
                    msg_content.append({
                        "type": "image",
                        "image": image_path
                    })
                
                msg_content.append({
                    "type": "text",
                    "text": content
                })
                
                messages.append({
                    "role": "user",
                    "content": msg_content
                })
                
            elif role == "assistant":
                # å¢å¼ºäººæƒ…å‘³
                enhanced_content = self.enhance_response_with_empathy(content)
                
                # æ£€æŸ¥è´¨é‡
                penalty, issues = self.check_response_quality(enhanced_content)
                
                messages.append({
                    "role": "assistant",
                    "content": enhanced_content
                })
        
        return {
            "messages": messages,
            "image_path": image_path
        }
    
    def process_dataset(self, dataset_name: str) -> List[Dict]:
        """
        å¤„ç†å•ä¸ªæ•°æ®é›†
        """
        console.print(f"\n[bold blue]ğŸ“¦ å¤„ç†æ•°æ®é›†: {dataset_name}[/bold blue]")
        
        data_path = self.raw_dir / dataset_name / "data.json"
        if not data_path.exists():
            console.print(f"[red]âŒ æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {data_path}[/red]")
            return []
        
        with open(data_path, "r", encoding="utf-8") as f:
            raw_data = json.load(f)
        
        processed_data = []
        image_output_dir = self.processed_dir / "images" / dataset_name
        image_output_dir.mkdir(parents=True, exist_ok=True)
        
        with Progress(
            SpinnerColumn(),
            TextColumn("[progress.description]{task.description}"),
            BarColumn(),
            TextColumn("[progress.percentage]{task.percentage:>3.0f}%"),
        ) as progress:
            task = progress.add_task(f"å¤„ç† {dataset_name}...", total=len(raw_data))
            
            for i, item in enumerate(raw_data):
                # å¤„ç†å›¾åƒ
                processed_image_path = None
                if item.get("image_path"):
                    output_img_path = str(image_output_dir / f"img_{i}.jpg")
                    processed_image_path = self.process_image(
                        item["image_path"], 
                        output_img_path
                    )
                
                # å¤„ç†å¯¹è¯
                if item.get("conversations"):
                    formatted = self.format_conversation_for_training(
                        item["conversations"],
                        processed_image_path
                    )
                    
                    processed_data.append({
                        "id": item.get("id", f"{dataset_name}_{i}"),
                        **formatted,
                        "metadata": item.get("metadata", {}),
                        "source": dataset_name
                    })
                
                progress.update(task, advance=1)
        
        console.print(f"[green]âœ… å¤„ç†å®Œæˆ: {len(processed_data)} æ¡æ•°æ®[/green]")
        return processed_data
    
    def create_train_val_split(
        self, 
        data: List[Dict], 
        val_ratio: float = 0.1
    ) -> Tuple[List[Dict], List[Dict]]:
        """
        åˆ›å»ºè®­ç»ƒ/éªŒè¯é›†åˆ’åˆ†
        """
        random.shuffle(data)
        split_idx = int(len(data) * (1 - val_ratio))
        return data[:split_idx], data[split_idx:]
    
    def save_processed_data(self, train_data: List[Dict], val_data: List[Dict]):
        """
        ä¿å­˜å¤„ç†åçš„æ•°æ®
        """
        # ä¿å­˜è®­ç»ƒæ•°æ®
        train_file = self.processed_dir / "train.json"
        with open(train_file, "w", encoding="utf-8") as f:
            json.dump(train_data, f, ensure_ascii=False, indent=2)
        
        # ä¿å­˜éªŒè¯æ•°æ®
        val_file = self.processed_dir / "val.json"
        with open(val_file, "w", encoding="utf-8") as f:
            json.dump(val_data, f, ensure_ascii=False, indent=2)
        
        # ä¿å­˜JSONLæ ¼å¼ï¼ˆç”¨äºè®­ç»ƒï¼‰
        train_jsonl = self.processed_dir / "train.jsonl"
        with open(train_jsonl, "w", encoding="utf-8") as f:
            for item in train_data:
                f.write(json.dumps(item, ensure_ascii=False) + "\n")
        
        val_jsonl = self.processed_dir / "val.jsonl"
        with open(val_jsonl, "w", encoding="utf-8") as f:
            for item in val_data:
                f.write(json.dumps(item, ensure_ascii=False) + "\n")
        
        console.print(f"[green]âœ… æ•°æ®å·²ä¿å­˜åˆ° {self.processed_dir}[/green]")
        console.print(f"   è®­ç»ƒé›†: {len(train_data)} æ¡")
        console.print(f"   éªŒè¯é›†: {len(val_data)} æ¡")
    
    def generate_penalty_reward_data(self) -> List[Dict]:
        """
        ç”Ÿæˆç”¨äºæƒ©ç½šæœºåˆ¶çš„å¯¹æ¯”æ•°æ®
        å±•ç¤ºå¥½çš„å›å¤å’Œå·®çš„å›å¤
        """
        console.print("\n[bold blue]ğŸ“ ç”Ÿæˆæƒ©ç½š/å¥–åŠ±å¯¹æ¯”æ•°æ®...[/bold blue]")
        
        contrast_pairs = [
            {
                "question": "æˆ‘çš„è¡€å‹æœ‰ç‚¹é«˜ï¼Œæ€ä¹ˆåŠï¼Ÿ",
                "good_response": "æˆ‘ç†è§£æ‚¨çš„æ‹…å¿§ï¼Œè¡€å‹åé«˜ç¡®å®éœ€è¦å…³æ³¨ã€‚é¦–å…ˆï¼Œè¯·ä¸è¦è¿‡åº¦ç„¦è™‘ï¼Œå› ä¸ºè½»åº¦é«˜è¡€å‹æ˜¯å¯ä»¥é€šè¿‡ç”Ÿæ´»æ–¹å¼è°ƒæ•´æ¥æ”¹å–„çš„ã€‚å»ºè®®æ‚¨ï¼š1) å‡å°‘ç›åˆ†æ‘„å…¥ 2) ä¿æŒè§„å¾‹è¿åŠ¨ 3) æ§åˆ¶ä½“é‡ 4) ä¿è¯å……è¶³ç¡çœ ã€‚å¦‚æœæŒç»­åé«˜ï¼Œå»ºè®®å’¨è¯¢å¿ƒå†…ç§‘åŒ»ç”Ÿã€‚æ‚¨æœ‰å…¶ä»–æƒ³äº†è§£çš„å—ï¼Ÿ",
                "bad_response": "è¡€å‹é«˜å°±æ˜¯é«˜è¡€å‹ï¼Œå¿…é¡»åƒè¯ï¼Œä½ å»åŒ»é™¢çœ‹çœ‹å§ã€‚"
            },
            {
                "question": "æ£€æŸ¥æŠ¥å‘Šæ˜¾ç¤ºæœ‰ç»“èŠ‚ï¼Œæ˜¯ä¸æ˜¯ç™Œç—‡ï¼Ÿ",
                "good_response": "æˆ‘èƒ½æ·±æ·±ç†è§£æ‚¨æ­¤åˆ»çš„æ‹…å¿§ï¼Œå‘ç°ç»“èŠ‚ç¡®å®ä¼šè®©äººæ„Ÿåˆ°ææƒ§ã€‚ä½†è¯·è®©æˆ‘å¸®æ‚¨ç†æ€§åœ°åˆ†æä¸€ä¸‹ï¼šé¦–å…ˆï¼Œå¤§å¤šæ•°ç»“èŠ‚éƒ½æ˜¯è‰¯æ€§çš„ï¼Œæ¶æ€§çš„æ¦‚ç‡å…¶å®å¾ˆä½ã€‚å…³é”®æ˜¯è¦çœ‹ç»“èŠ‚çš„å¤§å°ã€å½¢æ€ã€è¾¹ç¼˜ç‰¹å¾ç­‰ã€‚å»ºè®®æ‚¨ï¼š1) ä¸è¦è¿‡åº¦ææ…Œ 2) å’¨è¯¢ä¸“ç§‘åŒ»ç”Ÿçš„æ„è§ 3) å¦‚æœåŒ»ç”Ÿå»ºè®®éšè®¿è§‚å¯Ÿï¼Œè¯·æŒ‰æ—¶å¤æŸ¥ã€‚æœ‰ä»€ä¹ˆå…·ä½“é—®é¢˜æˆ‘å¯ä»¥å¸®æ‚¨è§£ç­”ï¼Ÿ",
                "bad_response": "è‚¯å®šæ˜¯è¦å»æ£€æŸ¥çš„ï¼Œè¿™ç§äº‹æƒ…è°è¯´å¾—å‡†å‘¢ï¼Œä½ è‡ªå·±æƒ³åŠæ³•å§ã€‚"
            },
            {
                "question": "æˆ‘ç»å¸¸å¤´ç—›ï¼Œéœ€è¦åšä»€ä¹ˆæ£€æŸ¥ï¼Ÿ",
                "good_response": "æ„Ÿè°¢æ‚¨å‘æˆ‘å’¨è¯¢ã€‚å¤´ç—›æ˜¯å¾ˆå¸¸è§çš„ç—‡çŠ¶ï¼Œæˆ‘ç†è§£è¿™ç»™æ‚¨å¸¦æ¥çš„ä¸é€‚å’Œå›°æ‰°ã€‚å¤´ç—›çš„åŸå› æœ‰å¾ˆå¤šï¼Œå¯èƒ½ä¸ç¡çœ ã€å‹åŠ›ã€é¢ˆæ¤é—®é¢˜ç­‰æœ‰å…³ã€‚ä¸ºäº†æ›´å¥½åœ°äº†è§£åŸå› ï¼Œå»ºè®®æ‚¨è®°å½•ä¸€ä¸‹ï¼š1) å¤´ç—›çš„éƒ¨ä½å’Œæ€§è´¨ 2) å‘ä½œé¢‘ç‡å’ŒæŒç»­æ—¶é—´ 3) æ˜¯å¦æœ‰ä¼´éšç—‡çŠ¶ã€‚å¦‚æœå¤´ç—›é¢‘ç¹æˆ–å‰§çƒˆï¼Œå¯ä»¥è€ƒè™‘åšå¤´éƒ¨CTæˆ–MRIæ£€æŸ¥ã€‚æ‚¨æ–¹ä¾¿æè¿°ä¸€ä¸‹å¤´ç—›çš„å…·ä½“æƒ…å†µå—ï¼Ÿ",
                "bad_response": "å¤´ç—›æ²¡ä»€ä¹ˆå¤§ä¸äº†çš„ï¼Œåƒç‚¹æ­¢ç—›è¯å°±è¡Œäº†ã€‚"
            }
        ]
        
        data_list = []
        for i, pair in enumerate(contrast_pairs):
            # å¥½çš„å›å¤
            data_list.append({
                "id": f"contrast_good_{i}",
                "messages": [
                    {"role": "user", "content": [{"type": "text", "text": pair["question"]}]},
                    {"role": "assistant", "content": pair["good_response"]}
                ],
                "image_path": None,
                "reward_label": 1.0,  # å¥–åŠ±
                "source": "contrast_positive"
            })
            
            # å·®çš„å›å¤ï¼ˆç”¨äºå¯¹æ¯”å­¦ä¹ ï¼‰
            data_list.append({
                "id": f"contrast_bad_{i}",
                "messages": [
                    {"role": "user", "content": [{"type": "text", "text": pair["question"]}]},
                    {"role": "assistant", "content": pair["bad_response"]}
                ],
                "image_path": None,
                "reward_label": -1.0,  # æƒ©ç½š
                "source": "contrast_negative"
            })
        
        console.print(f"[green]âœ… ç”Ÿæˆ {len(data_list)} æ¡å¯¹æ¯”æ•°æ®[/green]")
        return data_list


def main():
    """ä¸»å‡½æ•°"""
    console.print(Panel.fit(
        "[bold green]ğŸ”§ åŒ»ç–—æ•°æ®é¢„å¤„ç†[/bold green]\n"
        "å¤„ç†ä¸‹è½½çš„æ•°æ®é›†ï¼Œå‡†å¤‡è®­ç»ƒæ ¼å¼",
        border_style="green"
    ))
    
    # åˆ›å»ºç›®å½•
    create_directories()
    
    # åˆå§‹åŒ–é¢„å¤„ç†å™¨
    preprocessor = DataPreprocessor()
    
    all_data = []
    
    # å¤„ç†å„ä¸ªæ•°æ®é›†
    datasets_to_process = [
        "medical_vision_llm",
        "aquiles_medical_vision", 
        "medtrinity",
        "empathy_data"
    ]
    
    for dataset_name in datasets_to_process:
        data = preprocessor.process_dataset(dataset_name)
        all_data.extend(data)
    
    # ç”Ÿæˆå¯¹æ¯”æ•°æ®
    contrast_data = preprocessor.generate_penalty_reward_data()
    all_data.extend(contrast_data)
    
    # ç”Ÿæˆè¯ç‰©çŸ¥è¯†åº“è®­ç»ƒæ•°æ®
    console.print("\n[bold blue]ğŸ’Š ç”Ÿæˆè¯ç‰©çŸ¥è¯†åº“è®­ç»ƒæ•°æ®...[/bold blue]")
    try:
        from generate_drug_training_data import DrugTrainingDataGenerator
        drug_generator = DrugTrainingDataGenerator()
        drug_data = drug_generator.generate_all_training_data()
        all_data.extend(drug_data)
        console.print(f"[green]âœ… æ·»åŠ è¯ç‰©è®­ç»ƒæ•°æ®: {len(drug_data)} æ¡[/green]")
    except Exception as e:
        console.print(f"[yellow]âš ï¸  è¯ç‰©æ•°æ®ç”Ÿæˆå¤±è´¥: {str(e)}[/yellow]")
    
    # åˆ’åˆ†è®­ç»ƒ/éªŒè¯é›†
    console.print("\n[bold blue]ğŸ“Š åˆ’åˆ†è®­ç»ƒ/éªŒè¯é›†...[/bold blue]")
    train_data, val_data = preprocessor.create_train_val_split(all_data, val_ratio=0.1)
    
    # ä¿å­˜æ•°æ®
    preprocessor.save_processed_data(train_data, val_data)
    
    # æ€»ç»“
    console.print("\n" + "="*60)
    console.print(Panel.fit(
        f"[bold green]ğŸ“Š é¢„å¤„ç†å®Œæˆæ€»ç»“[/bold green]\n"
        f"æ€»æ•°æ®é‡: {len(all_data)} æ¡\n"
        f"è®­ç»ƒé›†: {len(train_data)} æ¡\n"
        f"éªŒè¯é›†: {len(val_data)} æ¡\n"
        f"è¾“å‡ºç›®å½•: {PROCESSED_DATA_DIR}",
        border_style="green"
    ))


if __name__ == "__main__":
    main()

