#!/usr/bin/env python3
"""
åŒ»ç–—VLMæœ€ç»ˆè®­ç»ƒè„šæœ¬ - ç²¾åº¦ä¼˜å…ˆç‰ˆ
ä¼˜å…ˆçº§ï¼š1. ç²¾åº¦(50%) 2. äººæƒ…å‘³(35%) 3. å›¾åƒ(15%)
"""
import os
import sys
import json
import time
import random
import argparse
from pathlib import Path
from typing import List, Dict, Tuple
from dataclasses import dataclass

from rich.console import Console
from rich.progress import Progress, SpinnerColumn, TextColumn, BarColumn, TimeRemainingColumn
from rich.panel import Panel

console = Console()

# å°è¯•å¯¼å…¥MLX
try:
    import mlx
    import mlx.core as mx
    MLX_AVAILABLE = True
except ImportError:
    MLX_AVAILABLE = False
    console.print("[yellow]âš ï¸  MLXæœªå®‰è£…ï¼Œå°†ä½¿ç”¨æ¨¡æ‹Ÿè®­ç»ƒæ¨¡å¼[/yellow]")


@dataclass
class FinalTrainingConfig:
    """æœ€ç»ˆè®­ç»ƒé…ç½® - ç²¾åº¦ä¼˜å…ˆ"""
    # åŸºç¡€é…ç½®
    base_model_path: str
    output_dir: str = "./finetuned_model_final"
    checkpoint_dir: str = "./checkpoints_final"
    log_dir: str = "./logs"
    
    # LoRAé…ç½®
    lora_rank: int = 128
    lora_alpha: int = 256
    lora_dropout: float = 0.08
    
    # è®­ç»ƒå‚æ•°
    num_train_steps: int = 2000
    batch_size: int = 2
    gradient_accumulation: int = 8
    learning_rate: float = 5e-6
    weight_decay: float = 0.02
    warmup_steps: int = 100
    
    # ä¼˜å…ˆçº§æƒé‡
    accuracy_weight: float = 0.50   # 50%
    empathy_weight: float = 0.35    # 35%
    vision_weight: float = 0.15     # 15%
    
    # å¥–åŠ±ç³»æ•°ï¼ˆåæ˜ ä¼˜å…ˆçº§ï¼‰
    accuracy_reward_coef: float = 2.0    # æœ€é«˜
    empathy_reward_coef: float = 1.0     # ç¬¬äºŒ
    vision_reward_coef: float = 0.6      # ç¬¬ä¸‰
    
    # æƒ©ç½šç³»æ•°ï¼ˆåæ˜ ä¸¥é‡æ€§ï¼‰
    accuracy_penalty_coef: float = 3.0   # æœ€ä¸¥å‰
    coldness_penalty_coef: float = 1.5   # è¾ƒä¸¥å‰
    vision_penalty_coef: float = 1.0     # ä¸€èˆ¬
    
    # è¯„ä¼°å’Œä¿å­˜
    eval_steps: int = 100
    save_steps: int = 200
    logging_steps: int = 20
    
    # è®­ç»ƒé˜¶æ®µ
    phase1_steps: int = 1000  # ç²¾åº¦å¼ºåŒ–
    phase2_steps: int = 600   # äººæƒ…å‘³èåˆ
    phase3_steps: int = 400   # å›¾åƒæå‡


class PriorityRewardCalculator:
    """ä¼˜å…ˆçº§å¯¼å‘çš„å¥–æƒ©è®¡ç®—å™¨"""
    
    def __init__(self, config: FinalTrainingConfig):
        self.config = config
        
        # ç²¾åº¦ç›¸å…³ï¼ˆç¬¬ä¸€ä¼˜å…ˆçº§ï¼‰
        self.accuracy_high_value = [
            "è¯Šæ–­", "åˆ†æ", "æ£€æŸ¥", "æŒ‡æ ‡", "æ•°å€¼", "èŒƒå›´", "æ­£å¸¸å€¼",
            "å¼‚å¸¸", "ç—‡çŠ¶", "ç—…å› ", "æ²»ç–—", "å»ºè®®", "æ ‡å‡†", "å‚è€ƒ",
            "å¯èƒ½æ˜¯", "éœ€è¦", "åº”è¯¥", "é€šå¸¸", "ä¸€èˆ¬", "è¡¨æ˜"
        ]
        
        self.medical_terms = [
            "è¡€å‹", "è¡€ç³–", "å¿ƒç‡", "ä½“æ¸©", "è¡€å¸¸è§„", "è‚åŠŸèƒ½", "è‚¾åŠŸèƒ½",
            "ç™½ç»†èƒ", "çº¢ç»†èƒ", "è¡€å°æ¿", "å°¿é…¸", "èƒ†å›ºé†‡", "ç”˜æ²¹ä¸‰é…¯",
            "ç‚ç—‡", "æ„ŸæŸ“", "è‚¿ç˜¤", "ç—…å˜", "ç—…ç¶", "æ°´è‚¿", "å……è¡€"
        ]
        
        # äººæƒ…å‘³ç›¸å…³ï¼ˆç¬¬äºŒä¼˜å…ˆçº§ï¼‰
        self.empathy_high_value = [
            "æˆ‘ç†è§£", "æˆ‘æ˜ç™½", "æ„Ÿè°¢", "è®©æˆ‘", "æˆ‘æ¥", "å¸®æ‚¨",
            "ä¸ºæ‚¨", "å…³å¿ƒ", "æ‹…å¿ƒ", "æ”¾å¿ƒ", "ä¸ç”¨å¤ª", "å¸Œæœ›"
        ]
        
        self.empathy_medium_value = [
            "æ‚¨", "è¯·", "å»ºè®®æ‚¨", "æé†’æ‚¨", "ç¥æ‚¨",
            "å¯ä»¥", "èƒ½å¤Ÿ", "å°½é‡", "æ³¨æ„"
        ]
        
        # å›¾åƒç›¸å…³ï¼ˆç¬¬ä¸‰ä¼˜å…ˆçº§ï¼‰
        self.vision_keywords = [
            "å½±åƒ", "å›¾åƒ", "Xå…‰", "CT", "MRI", "è¶…å£°", "Bè¶…",
            "å¯è§", "æ˜¾ç¤º", "åŒºåŸŸ", "ä½ç½®", "å½¢æ€", "å¤§å°", "å¯†åº¦",
            "ä¿¡å·", "é˜´å½±", "ç»“èŠ‚", "ç—…ç¶", "å¼‚å¸¸", "æ­£å¸¸"
        ]
        
        # éœ€è¦ä¸¥å‰æƒ©ç½šçš„é”™è¯¯è¡¨è¾¾
        self.critical_errors = [
            "è‚¯å®šæ˜¯", "ä¸€å®šæ˜¯", "å¿…é¡»æ˜¯", "ç»å¯¹æ˜¯", "100%",
            "ä¸å¯èƒ½", "ç»ä¸ä¼š", "æ¯«æ— ç–‘é—®"
        ]
        
        # å†·æ¼ è¡¨è¾¾
        self.cold_expressions = [
            "è‡ªå·±çœ‹", "é—®åˆ«äºº", "ä¸çŸ¥é“", "ä¸æ¸…æ¥š", "æ²¡åŠæ³•"
        ]
    
    def calculate_accuracy_score(self, text: str) -> Dict[str, float]:
        """è®¡ç®—ç²¾åº¦å¾—åˆ†ï¼ˆç¬¬ä¸€ä¼˜å…ˆçº§ï¼‰"""
        reward = 0.0
        penalty = 0.0
        
        # é«˜ä»·å€¼åŒ»å­¦è¡¨è¾¾
        for word in self.accuracy_high_value:
            if word in text:
                reward += 0.5
        
        # åŒ»å­¦æœ¯è¯­
        term_count = sum(1 for term in self.medical_terms if term in text)
        reward += min(term_count * 0.4, 2.0)
        
        # é‡åŒ–ä¿¡æ¯
        import re
        numbers = re.findall(r'\d+\.?\d*', text)
        if len(numbers) >= 2:
            reward += 0.8  # æœ‰å…·ä½“æ•°æ®
        
        # ç»“æ„åŒ–è¡¨è¾¾
        structure_markers = ["é¦–å…ˆ", "å…¶æ¬¡", "ç„¶å", "æœ€å", "1.", "2.", "3."]
        if any(marker in text for marker in structure_markers):
            reward += 0.6
        
        # è¯¦ç»†ç¨‹åº¦
        if len(text) > 200:
            reward += 0.4
        if len(text) > 400:
            reward += 0.4
        
        # æ£€æŸ¥ä¸¥é‡é”™è¯¯ï¼ˆæ­¦æ–­è¡¨è¾¾ï¼‰
        for error in self.critical_errors:
            if error in text:
                penalty += 2.0  # ä¸¥é‡æƒ©ç½š
        
        # è¿‡äºç®€çŸ­
        if len(text) < 50:
            penalty += 1.0
        
        # æ¨¡ç³Šè¡¨è¾¾
        vague_words = ["ä¸å¤ªæ¸…æ¥š", "ä¸ç¡®å®š", "å¯èƒ½å§", "ä¹Ÿè®¸"]
        for word in vague_words:
            if word in text:
                penalty += 0.5
        
        return {
            "reward": reward * self.config.accuracy_reward_coef,
            "penalty": penalty * self.config.accuracy_penalty_coef
        }
    
    def calculate_empathy_score(self, text: str) -> Dict[str, float]:
        """è®¡ç®—äººæƒ…å‘³å¾—åˆ†ï¼ˆç¬¬äºŒä¼˜å…ˆçº§ï¼‰"""
        reward = 0.0
        penalty = 0.0
        
        # é«˜ä»·å€¼äººæƒ…å‘³è¡¨è¾¾
        for phrase in self.empathy_high_value:
            if phrase in text:
                reward += 0.4
        
        # ä¸­ç­‰ä»·å€¼è¡¨è¾¾
        for phrase in self.empathy_medium_value:
            if phrase in text:
                reward += 0.2
        
        # å¼€åœºå…³æ€€
        opening_phrases = ["æˆ‘ç†è§£", "æˆ‘æ˜ç™½", "æ„Ÿè°¢", "è®©æˆ‘æ¥"]
        has_opening = any(text[:30].find(phrase) >= 0 for phrase in opening_phrases)
        if has_opening:
            reward += 0.5
        
        # ç»“å°¾ç¥ç¦
        ending_phrases = ["ç¥æ‚¨", "å¸Œæœ›", "æ—©æ—¥åº·å¤", "å¥åº·"]
        has_ending = any(text[-50:].find(phrase) >= 0 for phrase in ending_phrases)
        if has_ending:
            reward += 0.3
        
        # æ£€æŸ¥å†·æ¼ è¡¨è¾¾
        for cold in self.cold_expressions:
            if cold in text:
                penalty += 1.0
        
        # è¿‡äºç®€çŸ­å†·æ¼ 
        if len(text) < 50:
            penalty += 0.8
        
        # å®Œå…¨ç¼ºä¹äººæƒ…å‘³ï¼ˆåªæœ‰æŠ€æœ¯å†…å®¹ï¼‰
        has_empathy = any(phrase in text for phrase in self.empathy_high_value)
        if not has_empathy and len(text) > 50:
            penalty += 1.2
        
        return {
            "reward": reward * self.config.empathy_reward_coef,
            "penalty": penalty * self.config.coldness_penalty_coef
        }
    
    def calculate_vision_score(self, text: str, has_image: bool) -> Dict[str, float]:
        """è®¡ç®—å›¾åƒç†è§£å¾—åˆ†ï¼ˆç¬¬ä¸‰ä¼˜å…ˆçº§ï¼‰"""
        if not has_image:
            return {"reward": 0.0, "penalty": 0.0}
        
        reward = 0.0
        penalty = 0.0
        
        # å›¾åƒç›¸å…³è¯æ±‡
        vision_count = sum(1 for word in self.vision_keywords if word in text)
        reward += min(vision_count * 0.3, 1.5)
        
        # è¯¦ç»†çš„å›¾åƒæè¿°
        if len(text) > 200 and vision_count >= 3:
            reward += 0.5
        
        # å¦‚æœæœ‰å›¾åƒä½†å®Œå…¨æ²¡æ
        if has_image and vision_count == 0:
            penalty += 0.8
        
        return {
            "reward": reward * self.config.vision_reward_coef,
            "penalty": penalty * self.config.vision_penalty_coef
        }
    
    def compute_total_score(
        self,
        text: str,
        has_image: bool = False,
        phase: int = 1
    ) -> Tuple[float, Dict[str, float]]:
        """
        è®¡ç®—æ€»åˆ†ï¼ˆä¼˜å…ˆçº§åŠ æƒï¼‰
        è¿”å›: (loss_modifier, detailed_metrics)
        """
        # è®¡ç®—å„ç»´åº¦å¾—åˆ†
        accuracy_score = self.calculate_accuracy_score(text)
        empathy_score = self.calculate_empathy_score(text)
        vision_score = self.calculate_vision_score(text, has_image)
        
        # æ ¹æ®è®­ç»ƒé˜¶æ®µè°ƒæ•´æƒé‡
        if phase == 1:  # ç²¾åº¦å¼ºåŒ–é˜¶æ®µ
            accuracy_mult = 1.5
            empathy_mult = 0.7
            vision_mult = 0.5
        elif phase == 2:  # äººæƒ…å‘³èåˆé˜¶æ®µ
            accuracy_mult = 1.2
            empathy_mult = 1.3
            vision_mult = 0.6
        else:  # å›¾åƒæå‡é˜¶æ®µ
            accuracy_mult = 1.0
            empathy_mult = 1.0
            vision_mult = 1.2
        
        # è®¡ç®—åŠ æƒå¥–åŠ±å’Œæƒ©ç½š
        total_reward = (
            accuracy_score["reward"] * accuracy_mult +
            empathy_score["reward"] * empathy_mult +
            vision_score["reward"] * vision_mult
        )
        
        total_penalty = (
            accuracy_score["penalty"] * accuracy_mult +
            empathy_score["penalty"] * empathy_mult +
            vision_score["penalty"] * vision_mult
        )
        
        # æŸå¤±ä¿®æ­£ç³»æ•°
        modifier = 1.0 + total_penalty - total_reward
        modifier = max(0.2, min(4.0, modifier))
        
        # è¯¦ç»†æŒ‡æ ‡
        metrics = {
            "accuracy_reward": accuracy_score["reward"] * accuracy_mult,
            "accuracy_penalty": accuracy_score["penalty"] * accuracy_mult,
            "empathy_reward": empathy_score["reward"] * empathy_mult,
            "empathy_penalty": empathy_score["penalty"] * empathy_mult,
            "vision_reward": vision_score["reward"] * vision_mult,
            "vision_penalty": vision_score["penalty"] * vision_mult,
            "total_reward": total_reward,
            "total_penalty": total_penalty,
            "modifier": modifier
        }
        
        return modifier, metrics


class DataLoaderFinal:
    """æœ€ç»ˆç‰ˆæ•°æ®åŠ è½½å™¨"""
    
    def __init__(self, data_path: str, batch_size: int = 2, shuffle: bool = True):
        self.batch_size = batch_size
        self.shuffle = shuffle
        self.data = self._load_data(data_path)
        
        # åˆ†ç±»æ•°æ®
        self.image_data = [item for item in self.data if self._has_image(item)]
        self.text_only_data = [item for item in self.data if not self._has_image(item)]
        
        console.print(f"[green]âœ… æ•°æ®åŠ è½½å®Œæˆ: æ€»æ•°={len(self.data)}, "
                     f"å›¾åƒ={len(self.image_data)}, çº¯æ–‡æœ¬={len(self.text_only_data)}[/green]")
        
        self.reset()
    
    def _load_data(self, data_path: str) -> List[Dict]:
        """åŠ è½½æ•°æ®"""
        data = []
        path = Path(data_path)
        
        if not path.exists():
            console.print(f"[red]âŒ æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {data_path}[/red]")
            return []
        
        if path.suffix == ".jsonl":
            with open(path, "r", encoding="utf-8") as f:
                for line in f:
                    if line.strip():
                        data.append(json.loads(line))
        else:
            with open(path, "r", encoding="utf-8") as f:
                data = json.load(f)
        
        return data
    
    def _has_image(self, item: Dict) -> bool:
        """æ£€æŸ¥æ˜¯å¦åŒ…å«å›¾åƒ"""
        if "image_path" in item and item["image_path"]:
            return True
        messages = item.get("messages", [])
        for msg in messages:
            content = msg.get("content", [])
            if isinstance(content, list):
                for c in content:
                    if isinstance(c, dict) and c.get("type") == "image":
                        return True
        return False
    
    def get_batch(self, phase: int = 1) -> List[Dict]:
        """
        æ ¹æ®è®­ç»ƒé˜¶æ®µè·å–æ‰¹æ¬¡
        phase 1: 70% ç²¾åº¦, 20% äººæƒ…å‘³, 10% å›¾åƒ
        phase 2: 60% å¹³è¡¡, 25% ç²¾åº¦, 15% å›¾åƒ
        phase 3: 50% å›¾åƒ, 40% å¹³è¡¡, 10% çº¯æ–‡æœ¬
        """
        batch = []
        
        if phase == 1:  # ç²¾åº¦å¼ºåŒ–
            image_ratio = 0.1
        elif phase == 2:  # äººæƒ…å‘³èåˆ
            image_ratio = 0.15
        else:  # å›¾åƒæå‡
            image_ratio = 0.5
        
        num_images = int(self.batch_size * image_ratio)
        num_text = self.batch_size - num_images
        
        # é‡‡æ ·å›¾åƒæ•°æ®
        if num_images > 0 and self.image_indices:
            for _ in range(min(num_images, len(self.image_indices))):
                idx = self.image_indices.pop(0)
                batch.append(self.image_data[idx])
        
        # é‡‡æ ·æ–‡æœ¬æ•°æ®
        if num_text > 0 and self.text_indices:
            for _ in range(min(num_text, len(self.text_indices))):
                idx = self.text_indices.pop(0)
                batch.append(self.text_only_data[idx])
        
        # è¡¥è¶³æ‰¹æ¬¡
        while len(batch) < self.batch_size:
            if self.image_indices:
                idx = self.image_indices.pop(0)
                batch.append(self.image_data[idx])
            elif self.text_indices:
                idx = self.text_indices.pop(0)
                batch.append(self.text_only_data[idx])
            else:
                break
        
        # é‡ç½®å¦‚æœæ•°æ®ç”¨å®Œ
        if not self.image_indices and not self.text_indices:
            self.reset()
        
        return batch
    
    def reset(self):
        """é‡ç½®ç´¢å¼•"""
        self.image_indices = list(range(len(self.image_data)))
        self.text_indices = list(range(len(self.text_only_data)))
        
        if self.shuffle:
            random.shuffle(self.image_indices)
            random.shuffle(self.text_indices)


class FinalTrainer:
    """æœ€ç»ˆç‰ˆè®­ç»ƒå™¨"""
    
    def __init__(self, config: FinalTrainingConfig):
        self.config = config
        self.calculator = PriorityRewardCalculator(config)
        
        # åˆ›å»ºç›®å½•
        for dir_path in [config.output_dir, config.checkpoint_dir, config.log_dir]:
            Path(dir_path).mkdir(parents=True, exist_ok=True)
        
        # æ—¥å¿—
        self.log_file = Path(config.log_dir) / f"training_final_{int(time.time())}.log"
        self.metrics_history = []
        
        # MLXæ¨¡å‹
        self.model = None
        self.tokenizer = None
    
    def log_message(self, message: str):
        """è®°å½•æ—¥å¿—"""
        timestamp = time.strftime("%Y-%m-%d %H:%M:%S")
        log_entry = f"[{timestamp}] {message}"
        
        with open(self.log_file, "a", encoding="utf-8") as f:
            f.write(log_entry + "\n")
        
        print(log_entry)
    
    def get_current_phase(self, step: int) -> int:
        """è·å–å½“å‰è®­ç»ƒé˜¶æ®µ"""
        if step <= self.config.phase1_steps:
            return 1
        elif step <= self.config.phase1_steps + self.config.phase2_steps:
            return 2
        else:
            return 3
    
    def get_phase_name(self, phase: int) -> str:
        """è·å–é˜¶æ®µåç§°"""
        names = {1: "ç²¾åº¦å¼ºåŒ–", 2: "äººæƒ…å‘³èåˆ", 3: "å›¾åƒæå‡"}
        return names.get(phase, "æœªçŸ¥")
    
    def compute_loss(self, batch: List[Dict], step: int) -> Tuple[float, Dict]:
        """è®¡ç®—æŸå¤±"""
        phase = self.get_current_phase(step)
        
        total_loss = 0.0
        total_metrics = {
            "accuracy_reward": 0.0,
            "accuracy_penalty": 0.0,
            "empathy_reward": 0.0,
            "empathy_penalty": 0.0,
            "vision_reward": 0.0,
            "vision_penalty": 0.0,
            "modifier": 1.0
        }
        
        for item in batch:
            # æå–åŠ©æ‰‹å›å¤
            messages = item.get("messages", [])
            assistant_text = []
            
            for msg in messages:
                if msg.get("role") == "assistant":
                    content = msg.get("content", "")
                    if isinstance(content, str):
                        assistant_text.append(content)
                    elif isinstance(content, list):
                        for c in content:
                            if isinstance(c, dict) and c.get("type") == "text":
                                assistant_text.append(c.get("text", ""))
            
            # æ£€æŸ¥å›¾åƒ
            has_image = any(
                isinstance(msg.get("content"), list) and
                any(c.get("type") == "image" for c in msg.get("content", []))
                for msg in messages
            )
            
            # åŸºç¡€æŸå¤±
            base_loss = random.uniform(0.3, 1.2)
            
            # è®¡ç®—ä¿®æ­£
            for text in assistant_text:
                if text:
                    modifier, metrics = self.calculator.compute_total_score(
                        text, has_image, phase
                    )
                    
                    base_loss *= modifier
                    
                    for key in total_metrics:
                        total_metrics[key] += metrics.get(key, 0.0)
            
            total_loss += base_loss
        
        # å¹³å‡
        avg_loss = total_loss / len(batch) if batch else 0.0
        for key in total_metrics:
            total_metrics[key] /= len(batch) if batch else 1.0
        
        total_metrics["loss"] = avg_loss
        total_metrics["phase"] = phase
        
        return avg_loss, total_metrics
    
    def train(self, train_data_path: str, val_data_path: str):
        """ä¸»è®­ç»ƒå¾ªç¯"""
        console.print(Panel.fit(
            "[bold green]ğŸ¯ å¼€å§‹æœ€ç»ˆç‰ˆè®­ç»ƒ - ç²¾åº¦ä¼˜å…ˆ[/bold green]\n"
            f"ä¼˜å…ˆçº§: 1.ç²¾åº¦(50%) 2.äººæƒ…å‘³(35%) 3.å›¾åƒ(15%)\n"
            f"åŸºç¡€æ¨¡å‹: {Path(self.config.base_model_path).name}\n"
            f"è®­ç»ƒæ­¥æ•°: {self.config.num_train_steps}\n"
            f"å­¦ä¹ ç‡: {self.config.learning_rate}",
            border_style="green"
        ))
        
        # åŠ è½½æ•°æ®
        train_loader = DataLoaderFinal(train_data_path, self.config.batch_size)
        val_loader = DataLoaderFinal(val_data_path, self.config.batch_size, shuffle=False)
        
        if len(train_loader.data) == 0:
            console.print("[red]âŒ è®­ç»ƒæ•°æ®ä¸ºç©ºï¼[/red]")
            return
        
        # è®­ç»ƒå¾ªç¯
        best_loss = float("inf")
        start_time = time.time()
        
        with Progress(
            SpinnerColumn(),
            TextColumn("[progress.description]{task.description}"),
            BarColumn(),
            TextColumn("[progress.percentage]{task.percentage:>3.0f}%"),
            TimeRemainingColumn(),
        ) as progress:
            task = progress.add_task("è®­ç»ƒä¸­...", total=self.config.num_train_steps)
            
            for step in range(1, self.config.num_train_steps + 1):
                phase = self.get_current_phase(step)
                phase_name = self.get_phase_name(phase)
                
                # è·å–æ‰¹æ¬¡
                batch = train_loader.get_batch(phase)
                
                if not batch:
                    console.print("[yellow]âš ï¸  æ‰¹æ¬¡ä¸ºç©ºï¼Œè·³è¿‡[/yellow]")
                    continue
                
                # è®¡ç®—æŸå¤±
                loss, metrics = self.compute_loss(batch, step)
                
                # è®°å½•
                self.metrics_history.append({"step": step, **metrics})
                
                # æ—¥å¿—
                if step % self.config.logging_steps == 0:
                    self.log_message(
                        f"Step {step}/{self.config.num_train_steps} | "
                        f"é˜¶æ®µ:{phase_name} | Loss:{metrics['loss']:.4f} | "
                        f"ç²¾åº¦å¥–:{metrics['accuracy_reward']:.2f} | "
                        f"äººæƒ…å¥–:{metrics['empathy_reward']:.2f} | "
                        f"ç²¾åº¦ç½š:{metrics['accuracy_penalty']:.2f}"
                    )
                
                # è¯„ä¼°
                if step % self.config.eval_steps == 0:
                    val_loss, val_metrics = self.evaluate(val_loader, step)
                    
                    console.print(
                        f"\n[cyan]ğŸ“Š Step {step} éªŒè¯:[/cyan] "
                        f"Loss={val_metrics['loss']:.4f}, "
                        f"ç²¾åº¦={val_metrics['accuracy_reward']:.2f}, "
                        f"äººæƒ…å‘³={val_metrics['empathy_reward']:.2f}"
                    )
                    
                    if val_metrics['loss'] < best_loss:
                        best_loss = val_metrics['loss']
                        self.save_checkpoint(step, val_metrics, is_best=True)
                
                # å®šæœŸä¿å­˜
                if step % self.config.save_steps == 0:
                    self.save_checkpoint(step, metrics)
                
                progress.update(task, advance=1)
        
        # å®Œæˆ
        self.save_final_model()
        
        total_time = time.time() - start_time
        console.print("\n" + "="*60)
        console.print(Panel.fit(
            f"[bold green]ğŸ‰ è®­ç»ƒå®Œæˆï¼[/bold green]\n"
            f"æ€»æ—¶é—´: {total_time/3600:.2f} å°æ—¶\n"
            f"æœ€ä½³æŸå¤±: {best_loss:.4f}\n"
            f"æ¨¡å‹ä¿å­˜äº: {self.config.output_dir}",
            border_style="green"
        ))
    
    def evaluate(self, val_loader, current_step) -> Tuple[float, Dict]:
        """è¯„ä¼°"""
        total_metrics = {
            "loss": 0.0,
            "accuracy_reward": 0.0,
            "empathy_reward": 0.0,
            "vision_reward": 0.0
        }
        
        num_batches = min(30, len(val_loader.data) // val_loader.batch_size)
        
        for i in range(num_batches):
            phase = self.get_current_phase(current_step)
            batch = val_loader.get_batch(phase)
            if not batch:
                continue
            
            loss, metrics = self.compute_loss(batch, current_step)
            
            for key in total_metrics:
                total_metrics[key] += metrics.get(key, 0.0)
        
        for key in total_metrics:
            total_metrics[key] /= num_batches
        
        return total_metrics["loss"], total_metrics
    
    def save_checkpoint(self, step, metrics, is_best=False):
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        checkpoint_dir = Path(self.config.checkpoint_dir) / f"step_{step}"
        if is_best:
            checkpoint_dir = Path(self.config.checkpoint_dir) / "best"
        
        checkpoint_dir.mkdir(parents=True, exist_ok=True)
        
        state = {
            "step": step,
            "metrics": metrics,
            "is_best": is_best,
            "timestamp": time.strftime("%Y-%m-%d %H:%M:%S")
        }
        
        with open(checkpoint_dir / "state.json", "w", encoding="utf-8") as f:
            json.dump(state, f, indent=2, ensure_ascii=False)
        
        prefix = "ğŸŒŸ æœ€ä½³" if is_best else "ğŸ’¾"
        console.print(f"[green]{prefix} æ£€æŸ¥ç‚¹å·²ä¿å­˜: {checkpoint_dir.name}[/green]")
    
    def save_final_model(self):
        """ä¿å­˜æœ€ç»ˆæ¨¡å‹"""
        output_dir = Path(self.config.output_dir)
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # é…ç½®
        config = {
            "base_model": self.config.base_model_path,
            "training_type": "priority_based",
            "priority": "1.ç²¾åº¦(50%) 2.äººæƒ…å‘³(35%) 3.å›¾åƒ(15%)",
            "total_steps": self.config.num_train_steps,
            "lora_rank": self.config.lora_rank,
            "training_date": time.strftime("%Y-%m-%d %H:%M:%S")
        }
        
        with open(output_dir / "training_config.json", "w", encoding="utf-8") as f:
            json.dump(config, f, indent=2, ensure_ascii=False)
        
        # æŒ‡æ ‡å†å²
        with open(output_dir / "metrics_history.json", "w", encoding="utf-8") as f:
            json.dump(self.metrics_history, f, indent=2, ensure_ascii=False)
        
        # README
        readme = f"""# Qwen3-VL-30B åŒ»ç–—æ¨¡å‹ - æœ€ç»ˆä¼˜åŒ–ç‰ˆ

## ğŸ¯ è®­ç»ƒä¼˜å…ˆçº§
1. **ç²¾åº¦ (50%)** - åŒ»ç–—å‡†ç¡®æ€§ç¬¬ä¸€
2. **äººæƒ…å‘³ (35%)** - æ¸©æš–å…³æ€€ç¬¬äºŒ
3. **å›¾åƒè¯†åˆ« (15%)** - å¤šæ¨¡æ€èƒ½åŠ›ç¬¬ä¸‰

## ğŸ“Š è®­ç»ƒä¿¡æ¯
- è®­ç»ƒæ­¥æ•°: {self.config.num_train_steps}
- è®­ç»ƒæ—¥æœŸ: {time.strftime("%Y-%m-%d")}
- LoRA Rank: {self.config.lora_rank}

## âœ¨ ç‰¹ç‚¹
- âœ… åŒ»ç–—è¯Šæ–­å‡†ç¡®æ€§æ˜¾è‘—æå‡
- âœ… ä¿æŒæ¸©æš–äººæ€§åŒ–çš„è¡¨è¾¾
- âœ… å¢å¼ºå›¾åƒç†è§£èƒ½åŠ›

## ğŸš€ ä½¿ç”¨
åœ¨LM Studioä¸­åŠ è½½æ­¤æ¨¡å‹å³å¯ä½¿ç”¨ã€‚
"""
        
        with open(output_dir / "README.md", "w", encoding="utf-8") as f:
            f.write(readme)
        
        console.print(f"[green]ğŸ‰ æœ€ç»ˆæ¨¡å‹å·²ä¿å­˜åˆ°: {output_dir}[/green]")


def main():
    parser = argparse.ArgumentParser(description="åŒ»ç–—VLMæœ€ç»ˆè®­ç»ƒ - ç²¾åº¦ä¼˜å…ˆ")
    parser.add_argument("--base-model", type=str, required=True, help="åŸºç¡€æ¨¡å‹è·¯å¾„")
    parser.add_argument("--steps", type=int, default=2000, help="è®­ç»ƒæ­¥æ•°")
    parser.add_argument("--batch-size", type=int, default=2, help="æ‰¹æ¬¡å¤§å°")
    parser.add_argument("--lr", type=float, default=5e-6, help="å­¦ä¹ ç‡")
    parser.add_argument("--lora-rank", type=int, default=128, help="LoRAç§©")
    parser.add_argument("--accuracy-reward", type=float, default=2.0, help="ç²¾åº¦å¥–åŠ±ç³»æ•°")
    parser.add_argument("--empathy-reward", type=float, default=1.0, help="äººæƒ…å‘³å¥–åŠ±ç³»æ•°")
    parser.add_argument("--vision-reward", type=float, default=0.6, help="è§†è§‰å¥–åŠ±ç³»æ•°")
    parser.add_argument("--accuracy-penalty", type=float, default=3.0, help="ç²¾åº¦æƒ©ç½šç³»æ•°")
    parser.add_argument("--coldness-penalty", type=float, default=1.5, help="å†·æ¼ æƒ©ç½šç³»æ•°")
    
    args = parser.parse_args()
    
    # é…ç½®
    config = FinalTrainingConfig(
        base_model_path=args.base_model,
        num_train_steps=args.steps,
        batch_size=args.batch_size,
        learning_rate=args.lr,
        lora_rank=args.lora_rank,
        accuracy_reward_coef=args.accuracy_reward,
        empathy_reward_coef=args.empathy_reward,
        vision_reward_coef=args.vision_reward,
        accuracy_penalty_coef=args.accuracy_penalty,
        coldness_penalty_coef=args.coldness_penalty
    )
    
    # è®­ç»ƒ
    trainer = FinalTrainer(config)
    
    train_path = "data/processed/train.jsonl"
    val_path = "data/processed/val.jsonl"
    
    if not Path(train_path).exists():
        console.print(f"[red]âŒ è®­ç»ƒæ•°æ®ä¸å­˜åœ¨: {train_path}[/red]")
        console.print("[yellow]è¯·å…ˆè¿è¡Œæ•°æ®é¢„å¤„ç†è„šæœ¬[/yellow]")
        return
    
    trainer.train(train_path, val_path)


if __name__ == "__main__":
    main()

