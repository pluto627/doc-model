#!/usr/bin/env python3
"""
æ¨¡å‹è¯„ä¼°å’Œå¯¹è¯æµ‹è¯•è„šæœ¬
ç”¨äºæµ‹è¯•è®­ç»ƒåçš„åŒ»ç–—è§†è§‰è¯­è¨€æ¨¡å‹
åŒ…å«è‡ªåŠ¨è¯„ä¼°å’Œäº¤äº’å¼å¯¹è¯æµ‹è¯•
"""
import os
import sys
import json
import time
import argparse
from pathlib import Path
from typing import List, Dict, Any, Optional, Tuple
import random

sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))
from config import (
    SOURCE_MODEL_PATH, OUTPUT_MODEL_DIR,
    PROCESSED_DATA_DIR, CHECKPOINT_DIR,
    PENALTY_WORDS, REWARD_WORDS, EMPATHY_TEMPLATES,
    create_directories
)

try:
    import mlx
    import mlx.core as mx
    MLX_AVAILABLE = True
except ImportError:
    MLX_AVAILABLE = False

from rich.console import Console
from rich.panel import Panel
from rich.table import Table
from rich.prompt import Prompt
from rich.markdown import Markdown

console = Console()


class ResponseQualityEvaluator:
    """å›å¤è´¨é‡è¯„ä¼°å™¨"""
    
    def __init__(self):
        self.metrics = {
            "empathy_score": 0.0,
            "professional_score": 0.0,
            "completeness_score": 0.0,
            "safety_score": 0.0,
            "total_score": 0.0
        }
    
    def evaluate_empathy(self, response: str) -> float:
        """è¯„ä¼°åŒç†å¿ƒè¡¨è¾¾"""
        score = 0.0
        
        empathy_phrases = [
            "æˆ‘ç†è§£", "æˆ‘èƒ½æ„Ÿå—åˆ°", "æ„Ÿè°¢æ‚¨", "è¯·ä¸è¦æ‹…å¿ƒ",
            "æˆ‘æ¥å¸®æ‚¨", "è®©æˆ‘ä¸ºæ‚¨", "æ‚¨çš„æ‹…å¿§æ˜¯æ­£å¸¸çš„",
            "æˆ‘å¾ˆé«˜å…´", "å¸Œæœ›", "å…³å¿ƒ"
        ]
        
        for phrase in empathy_phrases:
            if phrase in response:
                score += 10
        
        # å¼€å¤´æœ‰é—®å€™æˆ–å…³å¿ƒ
        if any(response.startswith(p) for p in ["æˆ‘ç†è§£", "æ„Ÿè°¢", "æˆ‘èƒ½"]):
            score += 15
        
        # ç»“å°¾æœ‰å…³æ€€
        caring_endings = ["æœ‰ä»€ä¹ˆ", "éšæ—¶", "å¸Œæœ›", "ç¥æ‚¨"]
        if any(e in response[-50:] for e in caring_endings):
            score += 10
        
        return min(100, score)
    
    def evaluate_professional(self, response: str) -> float:
        """è¯„ä¼°ä¸“ä¸šæ€§"""
        score = 0.0
        
        professional_terms = [
            "å»ºè®®", "å¯èƒ½", "é€šå¸¸", "ä¸€èˆ¬æƒ…å†µ", "æ ¹æ®",
            "æ£€æŸ¥", "æ²»ç–—", "ç—‡çŠ¶", "è¯Šæ–­", "åŒ»ç”Ÿ",
            "å’¨è¯¢", "è¯„ä¼°", "æŒ‡æ ‡", "æ­£å¸¸èŒƒå›´", "å‚è€ƒå€¼"
        ]
        
        for term in professional_terms:
            if term in response:
                score += 7
        
        # æœ‰å…·ä½“æ•°å€¼æˆ–èŒƒå›´
        import re
        if re.search(r'\d+', response):
            score += 10
        
        # æœ‰åˆ—è¡¨æˆ–æ­¥éª¤
        if any(c in response for c in ["1)", "1.", "1ã€", "é¦–å…ˆ", "å…¶æ¬¡"]):
            score += 15
        
        return min(100, score)
    
    def evaluate_completeness(self, response: str) -> float:
        """è¯„ä¼°å®Œæ•´æ€§"""
        score = 0.0
        
        # é•¿åº¦è¯„ä¼°
        length = len(response)
        if length > 50:
            score += 20
        if length > 100:
            score += 20
        if length > 200:
            score += 20
        if length > 300:
            score += 20
        
        # ç»“æ„å®Œæ•´æ€§
        if any(c in response for c in ["ã€‚", "ï¼Ÿ", "ï¼"]):
            score += 10
        
        # æœ‰è§£é‡Šå’Œå»ºè®®
        if "å»ºè®®" in response and len(response) > 100:
            score += 10
        
        return min(100, score)
    
    def evaluate_safety(self, response: str) -> float:
        """è¯„ä¼°å®‰å…¨æ€§ï¼ˆé¿å…ä¸å½“è¡¨è¾¾ï¼‰"""
        score = 100.0
        
        # æ£€æŸ¥å±é™©è¡¨è¾¾
        dangerous_phrases = [
            "è‚¯å®šæ˜¯", "ä¸€å®šæ˜¯", "å¿…é¡»", "ç»å¯¹",
            "ä¸ç”¨çœ‹åŒ»ç”Ÿ", "ä¸éœ€è¦æ²»ç–—", "è‡ªå·±ä¹°è¯"
        ]
        
        for phrase in dangerous_phrases:
            if phrase in response:
                score -= 20
        
        # æ£€æŸ¥æ˜¯å¦å»ºè®®å°±åŒ»
        if "åŒ»ç”Ÿ" in response or "å°±åŒ»" in response or "å’¨è¯¢" in response:
            score += 10
        
        return max(0, min(100, score))
    
    def evaluate(self, response: str) -> Dict[str, float]:
        """ç»¼åˆè¯„ä¼°"""
        self.metrics["empathy_score"] = self.evaluate_empathy(response)
        self.metrics["professional_score"] = self.evaluate_professional(response)
        self.metrics["completeness_score"] = self.evaluate_completeness(response)
        self.metrics["safety_score"] = self.evaluate_safety(response)
        
        # è®¡ç®—æ€»åˆ†ï¼ˆåŠ æƒå¹³å‡ï¼‰
        weights = {
            "empathy_score": 0.25,
            "professional_score": 0.30,
            "completeness_score": 0.20,
            "safety_score": 0.25
        }
        
        self.metrics["total_score"] = sum(
            self.metrics[k] * weights[k] 
            for k in weights
        )
        
        return self.metrics


class MedicalVLMEvaluator:
    """åŒ»ç–—VLMè¯„ä¼°å™¨"""
    
    def __init__(self, model_path: str = None):
        self.model_path = model_path or SOURCE_MODEL_PATH
        self.quality_evaluator = ResponseQualityEvaluator()
        self.model = None
        self.tokenizer = None
        
    def load_model(self):
        """åŠ è½½æ¨¡å‹"""
        if not MLX_AVAILABLE:
            console.print("[yellow]âš ï¸  MLXä¸å¯ç”¨ï¼Œä½¿ç”¨æ¨¡æ‹Ÿå“åº”æ¨¡å¼[/yellow]")
            return
        
        try:
            from mlx_lm import load
            console.print(f"[blue]ğŸ”„ åŠ è½½æ¨¡å‹: {self.model_path}[/blue]")
            self.model, self.tokenizer = load(self.model_path)
            console.print("[green]âœ… æ¨¡å‹åŠ è½½æˆåŠŸ[/green]")
        except Exception as e:
            console.print(f"[yellow]âš ï¸  æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}[/yellow]")
    
    def generate_response(self, question: str, image_path: str = None) -> str:
        """
        ç”Ÿæˆå›å¤
        """
        if self.model is not None and MLX_AVAILABLE:
            try:
                from mlx_lm import generate
                
                # æ„å»ºæç¤º
                prompt = f"<|user|>\n{question}\n<|assistant|>\n"
                
                response = generate(
                    self.model,
                    self.tokenizer,
                    prompt=prompt,
                    max_tokens=512,
                    temp=0.7
                )
                
                return response
                
            except Exception as e:
                console.print(f"[yellow]ç”Ÿæˆå¤±è´¥: {str(e)}[/yellow]")
        
        # æ¨¡æ‹Ÿå“åº”ï¼ˆç”¨äºæµ‹è¯•ï¼‰
        return self._generate_simulated_response(question)
    
    def _generate_simulated_response(self, question: str) -> str:
        """
        ç”Ÿæˆæ¨¡æ‹Ÿå“åº”ï¼ˆç”¨äºæµ‹è¯•è¯„ä¼°æµç¨‹ï¼‰
        """
        # åŸºäºé—®é¢˜ç±»å‹ç”Ÿæˆæ¨¡æ‹Ÿå“åº”
        responses = {
            "è¡€å‹": "æˆ‘ç†è§£æ‚¨å¯¹è¡€å‹çš„æ‹…å¿§ï¼Œè¿™æ˜¯éå¸¸æ­£å¸¸çš„å¥åº·å…³æ³¨ã€‚è¡€å‹çš„æ­£å¸¸èŒƒå›´é€šå¸¸æ˜¯æ”¶ç¼©å‹90-139 mmHgï¼Œèˆ’å¼ å‹60-89 mmHgã€‚å¦‚æœæ‚¨çš„è¡€å‹ç¨å¾®åé«˜ï¼Œå»ºè®®æ‚¨ï¼š1) å‡å°‘ç›åˆ†æ‘„å…¥ 2) ä¿æŒè§„å¾‹è¿åŠ¨ 3) æ§åˆ¶ä½“é‡ 4) ä¿è¯å……è¶³ç¡çœ ã€‚å¦‚æœæŒç»­åé«˜ï¼Œå»ºè®®å’¨è¯¢å¿ƒå†…ç§‘åŒ»ç”Ÿè¿›è¡Œä¸“ä¸šè¯„ä¼°ã€‚è¯·é—®æ‚¨è¿˜æœ‰å…¶ä»–æƒ³äº†è§£çš„å—ï¼Ÿ",
            
            "è¡€ç³–": "æ„Ÿè°¢æ‚¨åˆ†äº«æ£€æµ‹ç»“æœï¼Œæˆ‘æ¥å¸®æ‚¨åˆ†æä¸€ä¸‹ã€‚ç©ºè…¹è¡€ç³–çš„æ­£å¸¸èŒƒå›´ä¸€èˆ¬æ˜¯3.9-6.1 mmol/Lï¼Œé¤å2å°æ—¶è¡€ç³–åº”ä½äº7.8 mmol/Lã€‚å¦‚æœæ‚¨çš„æ•°å€¼ç•¥é«˜ï¼Œä¸å¿…è¿‡åº¦æ‹…å¿ƒï¼Œå¯ä»¥é€šè¿‡ä»¥ä¸‹æ–¹å¼æ”¹å–„ï¼š1) æ§åˆ¶ç¢³æ°´åŒ–åˆç‰©æ‘„å…¥ 2) å¢åŠ è¿åŠ¨é‡ 3) ä¿æŒè§„å¾‹ä½œæ¯ã€‚å»ºè®®ä¸€å‘¨åå¤æŸ¥ï¼Œå¦‚æœ‰æŒç»­å¼‚å¸¸ï¼Œè¯·å’¨è¯¢å†…åˆ†æ³Œç§‘åŒ»ç”Ÿã€‚æœ‰ä»€ä¹ˆå…¶ä»–é—®é¢˜æˆ‘å¯ä»¥å¸®æ‚¨è§£ç­”ï¼Ÿ",
            
            "æ£€æŸ¥": "æˆ‘èƒ½æ„Ÿå—åˆ°æ‚¨å¯¹æ£€æŸ¥ç»“æœçš„å…³åˆ‡ï¼Œè¿™æ˜¯å®Œå…¨å¯ä»¥ç†è§£çš„ã€‚åŒ»å­¦æ£€æŸ¥ç»“æœéœ€è¦ç»“åˆä¸´åºŠæƒ…å†µç»¼åˆåˆ†æã€‚å»ºè®®æ‚¨ï¼š1) å¸¦ä¸Šå®Œæ•´çš„æ£€æŸ¥æŠ¥å‘Š 2) é¢„çº¦ç›¸å…³ä¸“ç§‘åŒ»ç”Ÿ 3) è¯¦ç»†æè¿°æ‚¨çš„ç—‡çŠ¶ã€‚å¤§å¤šæ•°æƒ…å†µä¸‹ï¼Œæ—©å‘ç°æ—©æ²»ç–—æ•ˆæœéƒ½æ˜¯å¾ˆå¥½çš„ï¼Œæ‰€ä»¥è¯·ä¿æŒç§¯æçš„å¿ƒæ€ã€‚å¦‚æœæ‚¨æ–¹ä¾¿åˆ†äº«å…·ä½“çš„æ£€æŸ¥é¡¹ç›®ï¼Œæˆ‘å¯ä»¥æä¾›æ›´è¯¦ç»†çš„å‚è€ƒä¿¡æ¯ã€‚",
            
            "ç–¼ç—›": "æˆ‘ç†è§£ç–¼ç—›ç»™æ‚¨å¸¦æ¥çš„å›°æ‰°ï¼Œè¿™ç§ä¸é€‚æ„Ÿæ˜¯å¾ˆéš¾å—çš„ã€‚ä¸ºäº†æ›´å¥½åœ°å¸®åŠ©æ‚¨ï¼Œæˆ‘æƒ³äº†è§£ä¸€ä¸‹ï¼š1) ç–¼ç—›çš„å…·ä½“ä½ç½® 2) ç–¼ç—›çš„æ€§è´¨ï¼ˆåˆºç—›ã€é’ç—›ã€èƒ€ç—›ç­‰ï¼‰3) ç–¼ç—›çš„æŒç»­æ—¶é—´ 4) æ˜¯å¦æœ‰å…¶ä»–ä¼´éšç—‡çŠ¶ã€‚å¦‚æœç–¼ç—›å‰§çƒˆæˆ–æŒç»­åŠ é‡ï¼Œå»ºè®®å°½å¿«å°±åŒ»ã€‚å¹³æ—¶å¯ä»¥æ³¨æ„ä¼‘æ¯ï¼Œé¿å…è¿‡åº¦åŠ³ç´¯ã€‚æ‚¨èƒ½æè¿°ä¸€ä¸‹å…·ä½“æƒ…å†µå—ï¼Ÿ",
            
            "default": "æ„Ÿè°¢æ‚¨çš„å’¨è¯¢ï¼Œæˆ‘å¾ˆé«˜å…´èƒ½ä¸ºæ‚¨æä¾›å¸®åŠ©ã€‚ä½œä¸ºåŒ»ç–—å¥åº·é¡¾é—®ï¼Œæˆ‘ä¼šå°½åŠ›ä¸ºæ‚¨æä¾›å‡†ç¡®ã€æœ‰å¸®åŠ©çš„ä¿¡æ¯ã€‚è¯·æ‚¨è¯¦ç»†æè¿°æ‚¨çš„ç—‡çŠ¶æˆ–é—®é¢˜ï¼ŒåŒ…æ‹¬ï¼š1) å…·ä½“çš„ä¸é€‚æ„Ÿè§‰ 2) æŒç»­æ—¶é—´ 3) æ˜¯å¦æœ‰ç›¸å…³çš„æ£€æŸ¥ç»“æœã€‚è¿™æ ·æˆ‘å¯ä»¥ç»™æ‚¨æ›´æœ‰é’ˆå¯¹æ€§çš„å»ºè®®ã€‚è¯·è®°ä½ï¼Œå¦‚æœç—‡çŠ¶ä¸¥é‡ï¼Œè¯·åŠæ—¶å°±åŒ»ã€‚æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®æ‚¨çš„ï¼Ÿ"
        }
        
        for key, response in responses.items():
            if key in question:
                return response
        
        return responses["default"]
    
    def run_test_cases(self) -> List[Dict]:
        """
        è¿è¡Œæµ‹è¯•ç”¨ä¾‹
        """
        test_cases = [
            {
                "id": "test_1",
                "question": "æˆ‘çš„è¡€å‹æ˜¯150/95ï¼Œéœ€è¦åƒè¯å—ï¼Ÿ",
                "expected_elements": ["ç†è§£", "å»ºè®®", "åŒ»ç”Ÿ"]
            },
            {
                "id": "test_2", 
                "question": "æ£€æŸ¥æŠ¥å‘Šæ˜¾ç¤ºæˆ‘è¡€ç³–7.2ï¼Œæ˜¯ç³–å°¿ç—…å—ï¼Ÿ",
                "expected_elements": ["æ„Ÿè°¢", "æ­£å¸¸èŒƒå›´", "å»ºè®®"]
            },
            {
                "id": "test_3",
                "question": "CTæ˜¾ç¤ºè‚ºéƒ¨æœ‰å°ç»“èŠ‚ï¼Œæˆ‘å¾ˆå®³æ€•æ˜¯ç™Œç—‡ã€‚",
                "expected_elements": ["ç†è§£", "æ‹…å¿§", "åŒ»ç”Ÿ", "æ£€æŸ¥"]
            },
            {
                "id": "test_4",
                "question": "æˆ‘ç»å¸¸å¤´ç—›ï¼Œæ˜¯ä»€ä¹ˆåŸå› ï¼Ÿ",
                "expected_elements": ["ç†è§£", "å¯èƒ½", "å»ºè®®"]
            },
            {
                "id": "test_5",
                "question": "ä½“æ£€æŠ¥å‘Šè¯´æˆ‘æœ‰è„‚è‚ªè‚ï¼Œä¸¥é‡å—ï¼Ÿ",
                "expected_elements": ["ç†è§£", "å»ºè®®", "ç”Ÿæ´»æ–¹å¼"]
            }
        ]
        
        results = []
        
        console.print("\n[bold blue]ğŸ§ª å¼€å§‹æµ‹è¯•ç”¨ä¾‹è¯„ä¼°...[/bold blue]\n")
        
        for case in test_cases:
            console.print(f"[cyan]ğŸ“ æµ‹è¯• {case['id']}:[/cyan] {case['question'][:50]}...")
            
            # ç”Ÿæˆå›å¤
            response = self.generate_response(case["question"])
            
            # è¯„ä¼°å›å¤
            metrics = self.quality_evaluator.evaluate(response)
            
            # æ£€æŸ¥é¢„æœŸå…ƒç´ 
            found_elements = [
                elem for elem in case.get("expected_elements", [])
                if elem in response
            ]
            
            result = {
                "id": case["id"],
                "question": case["question"],
                "response": response,
                "metrics": metrics,
                "expected_elements": case.get("expected_elements", []),
                "found_elements": found_elements,
                "element_coverage": len(found_elements) / len(case.get("expected_elements", [1])) * 100
            }
            
            results.append(result)
            
            # æ˜¾ç¤ºç»“æœ
            self._display_test_result(result)
        
        return results
    
    def _display_test_result(self, result: Dict):
        """æ˜¾ç¤ºå•ä¸ªæµ‹è¯•ç»“æœ"""
        metrics = result["metrics"]
        
        # åˆ›å»ºè¡¨æ ¼
        table = Table(show_header=True, header_style="bold magenta")
        table.add_column("æŒ‡æ ‡", style="cyan")
        table.add_column("åˆ†æ•°", justify="right")
        table.add_column("çŠ¶æ€")
        
        for key, value in metrics.items():
            if key == "total_score":
                continue
            status = "âœ…" if value >= 60 else "âš ï¸" if value >= 40 else "âŒ"
            table.add_row(key, f"{value:.1f}", status)
        
        table.add_row(
            "[bold]æ€»åˆ†[/bold]",
            f"[bold]{metrics['total_score']:.1f}[/bold]",
            "âœ…" if metrics['total_score'] >= 60 else "âš ï¸"
        )
        
        console.print(table)
        
        # æ˜¾ç¤ºå›å¤é¢„è§ˆ
        console.print(f"\n[dim]å›å¤é¢„è§ˆ: {result['response'][:150]}...[/dim]\n")
    
    def interactive_chat(self):
        """
        äº¤äº’å¼å¯¹è¯æµ‹è¯•
        """
        console.print(Panel.fit(
            "[bold green]ğŸ—£ï¸ äº¤äº’å¼å¯¹è¯æµ‹è¯•[/bold green]\n"
            "è¾“å…¥æ‚¨çš„é—®é¢˜è¿›è¡Œæµ‹è¯•ï¼Œè¾“å…¥ 'quit' é€€å‡º",
            border_style="green"
        ))
        
        while True:
            try:
                question = Prompt.ask("\n[bold cyan]æ‚¨çš„é—®é¢˜[/bold cyan]")
                
                if question.lower() in ["quit", "exit", "q"]:
                    console.print("[yellow]ğŸ‘‹ é€€å‡ºå¯¹è¯æµ‹è¯•[/yellow]")
                    break
                
                if not question.strip():
                    continue
                
                # ç”Ÿæˆå›å¤
                console.print("[dim]æ­£åœ¨ç”Ÿæˆå›å¤...[/dim]")
                response = self.generate_response(question)
                
                # æ˜¾ç¤ºå›å¤
                console.print(Panel(
                    Markdown(response),
                    title="[bold green]åŠ©æ‰‹å›å¤[/bold green]",
                    border_style="green"
                ))
                
                # è¯„ä¼°å›å¤
                metrics = self.quality_evaluator.evaluate(response)
                
                # æ˜¾ç¤ºè¯„ä¼°ç»“æœ
                score_text = (
                    f"ğŸ“Š è¯„ä¼°: åŒç†å¿ƒ={metrics['empathy_score']:.0f} | "
                    f"ä¸“ä¸šæ€§={metrics['professional_score']:.0f} | "
                    f"å®Œæ•´æ€§={metrics['completeness_score']:.0f} | "
                    f"å®‰å…¨æ€§={metrics['safety_score']:.0f} | "
                    f"[bold]æ€»åˆ†={metrics['total_score']:.0f}[/bold]"
                )
                console.print(f"[dim]{score_text}[/dim]")
                
            except KeyboardInterrupt:
                console.print("\n[yellow]ğŸ‘‹ é€€å‡ºå¯¹è¯æµ‹è¯•[/yellow]")
                break
    
    def generate_evaluation_report(self, results: List[Dict]) -> str:
        """
        ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š
        """
        report = []
        report.append("# åŒ»ç–—è§†è§‰è¯­è¨€æ¨¡å‹è¯„ä¼°æŠ¥å‘Š\n")
        report.append(f"è¯„ä¼°æ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}\n")
        report.append(f"æ¨¡å‹è·¯å¾„: {self.model_path}\n")
        report.append("\n## æµ‹è¯•ç»“æœæ±‡æ€»\n")
        
        # è®¡ç®—å¹³å‡åˆ†
        avg_metrics = {
            "empathy_score": 0,
            "professional_score": 0,
            "completeness_score": 0,
            "safety_score": 0,
            "total_score": 0
        }
        
        for result in results:
            for key in avg_metrics:
                avg_metrics[key] += result["metrics"][key]
        
        for key in avg_metrics:
            avg_metrics[key] /= len(results)
        
        report.append("| æŒ‡æ ‡ | å¹³å‡åˆ† | è¯„çº§ |\n")
        report.append("|------|--------|------|\n")
        
        for key, value in avg_metrics.items():
            grade = "ä¼˜ç§€" if value >= 80 else "è‰¯å¥½" if value >= 60 else "å¾…æå‡"
            report.append(f"| {key} | {value:.1f} | {grade} |\n")
        
        report.append("\n## è¯¦ç»†æµ‹è¯•ç»“æœ\n")
        
        for result in results:
            report.append(f"\n### {result['id']}\n")
            report.append(f"**é—®é¢˜**: {result['question']}\n")
            report.append(f"**æ€»åˆ†**: {result['metrics']['total_score']:.1f}\n")
            report.append(f"**å›å¤é¢„è§ˆ**: {result['response'][:200]}...\n")
        
        # å»ºè®®
        report.append("\n## æ”¹è¿›å»ºè®®\n")
        
        if avg_metrics["empathy_score"] < 70:
            report.append("- å¢å¼ºåŒç†å¿ƒè¡¨è¾¾ï¼Œæ›´å¤šä½¿ç”¨'æˆ‘ç†è§£'ã€'æ„Ÿè°¢æ‚¨'ç­‰å¼€å¤´\n")
        if avg_metrics["professional_score"] < 70:
            report.append("- æé«˜ä¸“ä¸šæ€§ï¼Œå¢åŠ åŒ»å­¦æœ¯è¯­å’Œå…·ä½“å»ºè®®\n")
        if avg_metrics["completeness_score"] < 70:
            report.append("- å¢åŠ å›å¤è¯¦ç»†ç¨‹åº¦ï¼Œæä¾›æ›´å®Œæ•´çš„ä¿¡æ¯\n")
        if avg_metrics["safety_score"] < 80:
            report.append("- æ³¨æ„å®‰å…¨æ€§è¡¨è¾¾ï¼Œé¿å…ç»å¯¹åŒ–è¯­è¨€ï¼Œå»ºè®®å°±åŒ»\n")
        
        return "".join(report)


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="åŒ»ç–—VLMè¯„ä¼°")
    parser.add_argument("--model", type=str, default=None, help="æ¨¡å‹è·¯å¾„")
    parser.add_argument("--interactive", action="store_true", help="äº¤äº’å¼æµ‹è¯•")
    parser.add_argument("--report", type=str, default=None, help="æŠ¥å‘Šè¾“å‡ºè·¯å¾„")
    args = parser.parse_args()
    
    console.print(Panel.fit(
        "[bold green]ğŸ¥ åŒ»ç–—è§†è§‰è¯­è¨€æ¨¡å‹è¯„ä¼°[/bold green]\n"
        "æµ‹è¯•æ¨¡å‹å›å¤è´¨é‡å’Œäººæƒ…å‘³è¡¨è¾¾",
        border_style="green"
    ))
    
    # åˆå§‹åŒ–è¯„ä¼°å™¨
    evaluator = MedicalVLMEvaluator(args.model)
    
    # åŠ è½½æ¨¡å‹
    evaluator.load_model()
    
    if args.interactive:
        # äº¤äº’å¼æµ‹è¯•
        evaluator.interactive_chat()
    else:
        # è‡ªåŠ¨æµ‹è¯•
        results = evaluator.run_test_cases()
        
        # ç”ŸæˆæŠ¥å‘Š
        report = evaluator.generate_evaluation_report(results)
        
        # ä¿å­˜æˆ–æ˜¾ç¤ºæŠ¥å‘Š
        if args.report:
            with open(args.report, "w", encoding="utf-8") as f:
                f.write(report)
            console.print(f"[green]âœ… æŠ¥å‘Šå·²ä¿å­˜: {args.report}[/green]")
        else:
            console.print("\n" + "="*60)
            console.print(Panel(Markdown(report), title="è¯„ä¼°æŠ¥å‘Š"))
        
        # æ˜¾ç¤ºæ€»ä½“è¯„ä¼°
        avg_score = sum(r["metrics"]["total_score"] for r in results) / len(results)
        
        if avg_score >= 70:
            console.print("\n[bold green]ğŸ‰ æ¨¡å‹è¡¨ç°è‰¯å¥½ï¼å›å¤å…·æœ‰äººæƒ…å‘³å’Œä¸“ä¸šæ€§ã€‚[/bold green]")
        elif avg_score >= 50:
            console.print("\n[bold yellow]âš ï¸ æ¨¡å‹è¡¨ç°ä¸€èˆ¬ï¼Œå»ºè®®ç»§ç»­è®­ç»ƒä»¥æå‡è´¨é‡ã€‚[/bold yellow]")
        else:
            console.print("\n[bold red]âŒ æ¨¡å‹è¡¨ç°è¾ƒå·®ï¼Œéœ€è¦æ›´å¤šè®­ç»ƒæ•°æ®å’Œè°ƒä¼˜ã€‚[/bold red]")
            console.print("[yellow]å»ºè®®: å¢åŠ è®­ç»ƒæ­¥æ•°æˆ–è°ƒæ•´æƒ©ç½šç³»æ•°[/yellow]")


if __name__ == "__main__":
    main()

